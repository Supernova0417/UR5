diff --git a/Final Project Instructions.pdf b/Final Project Instructions.pdf
deleted file mode 100644
index c046285..0000000
--- a/Final Project Instructions.pdf	
+++ /dev/null
@@ -1,83 +0,0 @@
-Final Project Instructions
-
-This document outlines the requirements and tasks for the final project. All necessary materials have been provided,
-including:
-
-    Report template
-    Code files containing the PPO example, SAC/DDPG/TD3 implementations, and the UR5 environment
-    A collection of related research papers in reference_papers.zip
-    A detailed PPO project description in Final_Project_PPO.pdf
-
-The objective of this project is to control a UR5 robotic manipulator to reach a target pose using reinforcement learning.
-Students are required to complete the following three components:
-
-1. PPO Implementation
-
-Students must work with the baseline PPO implementation and improve its performance by completing the following steps:
-
-    Modify the reward function in the UR5 environment.
-    Tune PPO hyperparameters to obtain stable and successful learning.
-    Analyze training results using metrics such as episodic return curves and success rates.
-
-For more detailed instructions on the PPO setup, training procedure, and reward configuration, please refer to
-Final_Project_PPO.pdf.
-
-2. Off-Policy Algorithm Implementation (Choose One)
-
-Select one of the following reinforcement learning algorithms:
-
-    SAC
-    DDPG
-    TD3
-
-Adapt the selected algorithm to work with the UR5 environment. (refer to ppo_continuous_action.py)
-Train the agent and evaluate its performance using appropriate metrics and qualitative observations.
-
-Note:
-Example implementations of SAC, DDPG, and TD3 are included in the provided code package.
-These files are located under the CleanRL directory:
-
-   cleanrl/cleanrl/sac_continuous_action.py
-   cleanrl/cleanrl/ddpg_continuous_action.py
-   cleanrl/cleanrl/td3_continuous_action.py
-
-Students should also refer to reference_papers.zip for the original research papers related to the chosen algorithm.
-
-3. Report
-A written report must follow the required structure provided in: final_report_{student_id}.docx
-The report should include:
-
-    Theoretical background (RL, PPO, and the chosen off-policy algorithm)
-    Implementation details (reward design, hyperparameters, algorithm-specific changes)
-    Experimental results (training curves, success rates, etc)
-    Discussion and conclusions
-
-Students need to use the papers in reference_papers.zip when writing the Background/Theory section and when comparing
-PPO with the chosen algorithm.
-
-Tips for Good Reports (Evaluation Criteria)
-
-Your report will be evaluated based on the following key criteria.
-Please ensure that each criterion is addressed clearly in your writing.
-
-1. Explanation of Methods
-
-You should clearly explain PPO and the chosen off-policy algorithm in your own words, including the main ideas and how
-they differ.
-
-2. Implementation Details
-
-Describe the important implementation decisions you made, including:
-
-    How you modified the reward function
-    Which hyperparameters you tuned
-    Why you made those choices
-
-3. Experimental Results & Analysis
-
-Include training curves and relevant performance metrics, and interpret the results.
-Explain:
-
-    How PPO and the chosen algorithm compare
-    Why certain behaviors or differences appeared
-
diff --git a/Final_Project_PPO (updated).pdf b/Final_Project_PPO (updated).pdf
deleted file mode 100644
index 3a7f62c..0000000
--- a/Final_Project_PPO (updated).pdf	
+++ /dev/null
@@ -1,109 +0,0 @@
-Final_Project_PPO
-
-   This project implements reinforcement learning (RL) for trajectory generation of a UR5 robot arm using the Proximal
-   Policy Optimization (PPO) algorithm.
-
-1. System Setup
-
-1.1 macOS Setup
-
-   brew install glfw
-   conda create -n ur5 python=3.10
-   conda activate ur5
-   pip install mujoco gymnasium numpy
-   pip install mujoco-python-viewer
-   pip install "dm_control>=1.0.0"
-   pip install matplotlib
-   cd cleanrl
-   pip install -r requirements/requirements.txt
-
-1.2 Windows Setup
-
-   conda create -n ur5 python=3.10
-   conda activate ur5
-   pip install glfw
-   pip install mujoco gymnasium numpy
-   pip install mujoco-python-viewer
-   pip install "dm_control>=1.0.0"
-   pip install matplotlib
-   cd cleanrl
-   pip install -r requirements/requirements.txt
-
-2. Running the Training Code
-
-2.1 Making the UR5 Environment Discoverable
-
-    Before training, ensure Python can locate your custom environment folder by adding it to PYTHONPATH .
-
-   cd ..
-   pwd # Example: /home/user/Desktop/UR5
-   export PYTHONPATH=<current path>:$PYTHONPATH
-   # e.g.) export PYTHONPATH=/home/user/Desktop:$PYTHONPATH
-2.2 Logging Training Data with Weights & Biases (wandb)
-
-We use **wandb** for experiment tracking.
-
-    Please sign up for W&B (https://wandb.ai)
-    Type the commend below for log in on terminal
-
-   wandb login
-
-    Paste your API key when prompted.
-
-2.3 Running PPO
-
-   python cleanrl/cleanrl/ppo_continuous_action.py
-
-3. Reward Function & Parameter Tuning
-
-You must modify the following two sections in env.py to achieve the desired behavior.
-
-3.1 Reward Configuration
-
-    Below is the editable reward configuration inside env.py.
-
-   self.reward_cfg = {
-          dist_success_thresh": 0.03,
-          "time_penalty": 0.01,
-          "collision_penalty": 10.0,
-          "success_bonus": 100.0,
-          "orient_weight": 0.1,
-          "orient_power": 2.0,
-          "progress_scale": 5.0,
-
-   }
-
-3.2 Reward Function
-
-          def _compute_reward(self, dist, prev_dist, collided: bool, success: bool):
-
-    The following reward function is only an example and may need modification to improve performance depending on the
-    task.
-    Typically, it can be composed of terms such as distance to the target, goal-reaching rewards, and time-delay penalties.
-    Each term can be designed using L1 loss, L2 loss, exponential functions, logarithmic functions, or other formulations
-    depending on the desired behavior.
-4. PPO Hyperparameter Tuning
-
-    You can modify the following inside ppo_continuous_action.py:
-    Key PPO Parameters
-
-            total_timesteps: Total training duration
-            num_envs: Parallel environments for faster sample collection
-            num_minibatches: Batch splitting for PPO updates
-            learning_rate: Controls policy/critic update speed
-            gamma: Discount factor
-    Neural Network Configuration
-          MLP hidden layers
-          Activation functions
-
-5. Result
-
-    Before running inference.py, please modify line 29 as shown below.
-
-   agent.load_state_dict(torch.load("runs/<your run
-   name>/ppo_continuous_action.cleanrl_model", map_location=device))
-
-    You can visualize the output by running inference.py.
-
-   python cleanrl/cleanrl/inference.py
-
diff --git a/final_report_{student_id}.docx b/final_report_{student_id}.docx
deleted file mode 100644
index 8f516ad..0000000
--- a/final_report_{student_id}.docx
+++ /dev/null
@@ -1,24 +0,0 @@
-1. Introduction
-
-2. Background (Theory Overview)
-Summarize the theoretical concepts needed to understand the rest of the report.
-2.1 Reinforcement Learning Overview
-2.2 Proximal Policy Optimization (PPO)
-2.3 SAC / DDPG / TD3 (Choose one)
-
-3. PPO Implementation
-3.1 Reward Function Modification
-3.2 PPO Hyperparameter Tuning
- PPO Results
- Training curves (Episodic Return), Success rates
-
-4. (SAC / DDPG / TD3) Implementation
-4.1 Algorithm Implementation Summary
-4.2 Hyperparameter Tuning
-4.3 Experimental Results
-
-5. Discussion
-Students should reflect on:
- Why certain rewards or hyperparameters worked better
- Algorithm trade-offs in continuous manipulation
- Suggestions for future work
diff --git a/reference papers/DDPG_paper.pdf b/reference papers/DDPG_paper.pdf
deleted file mode 100644
index 1847594..0000000
--- a/reference papers/DDPG_paper.pdf	
+++ /dev/null
@@ -1,884 +0,0 @@
-arXiv:1509.02971v6 [cs.LG] 5 Jul 2019  Published as a conference paper at ICLR 2016
-
-                                       CONTINUOUS CONTROL WITH DEEP REINFORCEMENT
-
-                                       LEARNING
-
-                                        Timothy P. Lillicrap,∗Jonathan J. Hunt,∗Alexander Pritzel, Nicolas Heess,
-                                        Tom Erez, Yuval Tassa, David Silver & Daan Wierstra
-                                        Google Deepmind
-                                        London, UK
-                                        {countzero, jjhunt, apritzel, heess,
-                                          etom, tassa, davidsilver, wierstra} @ google.com
-
-                                                                                  ABSTRACT
-
-                                                 We adapt the ideas underlying the success of Deep Q-Learning to the continuous
-                                                 action domain. We present an actor-critic, model-free algorithm based on the de-
-                                                 terministic policy gradient that can operate over continuous action spaces. Using
-                                                 the same learning algorithm, network architecture and hyper-parameters, our al-
-                                                 gorithm robustly solves more than 20 simulated physics tasks, including classic
-                                                 problems such as cartpole swing-up, dexterous manipulation, legged locomotion
-                                                 and car driving. Our algorithm is able to ﬁnd policies whose performance is com-
-                                                 petitive with those found by a planning algorithm with full access to the dynamics
-                                                 of the domain and its derivatives. We further demonstrate that for many of the
-                                                 tasks the algorithm can learn policies “end-to-end”: directly from raw pixel in-
-                                                 puts.
-
-                                       1 INTRODUCTION
-
-                                       One of the primary goals of the ﬁeld of artiﬁcial intelligence is to solve complex tasks from unpro-
-                                       cessed, high-dimensional, sensory input. Recently, signiﬁcant progress has been made by combin-
-                                       ing advances in deep learning for sensory processing (Krizhevsky et al., 2012) with reinforcement
-                                       learning, resulting in the “Deep Q Network” (DQN) algorithm (Mnih et al., 2015) that is capable of
-                                       human level performance on many Atari video games using unprocessed pixels for input. To do so,
-                                       deep neural network function approximators were used to estimate the action-value function.
-
-                                       However, while DQN solves problems with high-dimensional observation spaces, it can only handle
-                                       discrete and low-dimensional action spaces. Many tasks of interest, most notably physical control
-                                       tasks, have continuous (real valued) and high dimensional action spaces. DQN cannot be straight-
-                                       forwardly applied to continuous domains since it relies on a ﬁnding the action that maximizes the
-                                       action-value function, which in the continuous valued case requires an iterative optimization process
-                                       at every step.
-
-                                       An obvious approach to adapting deep reinforcement learning methods such as DQN to continuous
-                                       domains is to to simply discretize the action space. However, this has many limitations, most no-
-                                       tably the curse of dimensionality: the number of actions increases exponentially with the number
-                                       of degrees of freedom. For example, a 7 degree of freedom system (as in the human arm) with the
-                                       coarsest discretization ai ∈ {−k, 0, k} for each joint leads to an action space with dimensionality:
-                                       37 = 2187. The situation is even worse for tasks that require ﬁne control of actions as they require
-                                       a correspondingly ﬁner grained discretization, leading to an explosion of the number of discrete
-                                       actions. Such large action spaces are difﬁcult to explore efﬁciently, and thus successfully training
-                                       DQN-like networks in this context is likely intractable. Additionally, naive discretization of action
-                                       spaces needlessly throws away information about the structure of the action domain, which may be
-                                       essential for solving many problems.
-
-                                       In this work we present a model-free, off-policy actor-critic algorithm using deep function approx-
-                                       imators that can learn policies in high-dimensional, continuous action spaces. Our work is based
-
-                                          ∗These authors contributed equally.
-
-                                                                                                  1
-Published as a conference paper at ICLR 2016
-
-on the deterministic policy gradient (DPG) algorithm (Silver et al., 2014) (itself similar to NFQCA
-(Hafner & Riedmiller, 2011), and similar ideas can be found in (Prokhorov et al., 1997)). However,
-as we show below, a naive application of this actor-critic method with neural function approximators
-is unstable for challenging problems.
-
-Here we combine the actor-critic approach with insights from the recent success of Deep Q Network
-(DQN) (Mnih et al., 2013; 2015). Prior to DQN, it was generally believed that learning value
-functions using large, non-linear function approximators was difﬁcult and unstable. DQN is able
-to learn value functions using such function approximators in a stable and robust way due to two
-innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize
-correlations between samples; 2. the network is trained with a target Q network to give consistent
-targets during temporal difference backups. In this work we make use of the same ideas, along with
-batch normalization (Ioffe & Szegedy, 2015), a recent advance in deep learning.
-
-In order to evaluate our method we constructed a variety of challenging physical control problems
-that involve complex multi-joint movements, unstable and rich contact dynamics, and gait behavior.
-Among these are classic problems such as the cartpole swing-up problem, as well as many new
-domains. A long-standing challenge of robotic control is to learn an action policy directly from raw
-sensory input such as video. Accordingly, we place a ﬁxed viewpoint camera in the simulator and
-attempted all tasks using both low-dimensional observations (e.g. joint angles) and directly from
-pixels.
-
-Our model-free approach which we call Deep DPG (DDPG) can learn competitive policies for all of
-our tasks using low-dimensional observations (e.g. cartesian coordinates or joint angles) using the
-same hyper-parameters and network structure. In many cases, we are also able to learn good policies
-directly from pixels, again keeping hyperparameters and network structure constant 1.
-
-A key feature of the approach is its simplicity: it requires only a straightforward actor-critic archi-
-tecture and learning algorithm with very few “moving parts”, making it easy to implement and scale
-to more difﬁcult problems and larger networks. For the physical control problems we compare our
-results to a baseline computed by a planner (Tassa et al., 2012) that has full access to the underly-
-ing simulated dynamics and its derivatives (see supplementary information). Interestingly, DDPG
-can sometimes ﬁnd policies that exceed the performance of the planner, in some cases even when
-learning from pixels (the planner always plans over the underlying low-dimensional state space).
-
-2 BACKGROUND
-
-We consider a standard reinforcement learning setup consisting of an agent interacting with an en-
-
-vironment E in discrete timesteps. At each timestep t the agent receives an observation xt, takes
-an action at and receives a scalar reward rt. In all the environments considered here the actions
-are real-valued at ∈ IRN . In general, the environment may be partially observed so that the entire
-history of the observation, action pairs st = (x1, a1, ..., at−1, xt) may be required to describe the
-state. Here, we assumed the environment is fully-observed so st = xt.
-
-An agent’s behavior is deﬁned by a policy, π, which maps states to a probability distribution over
-
-the actions π : S → P(A). The environment, E, may also be stochastic. We model it as a Markov
-decision process with a state space S, action space A = IRN , an initial state distribution p(s1),
-transition dynamics p(st+1|st, at), and reward function r(st, at).
-
-The return from a state is deﬁned as the sum of discounted future reward Rt =  T    γ(i−t)r(si,  ai)
-                                                                               i=t
-with a discounting factor γ ∈ [0, 1]. Note that the return depends on the actions chosen, and therefore
-
-on the policy π, and may be stochastic. The goal in reinforcement learning is to learn a policy which
-
-maximizes the expected return from the start distribution J = Eri,si∼E,ai∼π [R1]. We denote the
-discounted state visitation distribution for a policy π as ρπ.
-
-The action-value function is used in many reinforcement learning algorithms. It describes the ex-
-pected return after taking an action at in state st and thereafter following policy π:
-
-              Qπ(st, at) = Eri≥t,si>t∼E,ai>t∼π [Rt|st, at]                                       (1)
-
-1You can view a movie of some of the learned policies at https://goo.gl/J4PIAz
-
-                                              2
-Published as a conference paper at ICLR 2016
-
-Many approaches in reinforcement learning make use of the recursive relationship known as the
-Bellman equation:
-
-              Qπ(st, at) = Ert,st+1∼E r(st, at) + γ Eat+1∼π [Qπ(st+1, at+1)]  (2)
-
-If the target policy is deterministic we can describe it as a function µ : S ← A and avoid the inner
-
-expectation:
-
-              Qµ(st, at) = Ert,st+1∼E [r(st, at) + γQµ(st+1, µ(st+1))]        (3)
-
-The expectation depends only on the environment. This means that it is possible to learn Qµ off-
-policy, using transitions which are generated from a different stochastic behavior policy β.
-
-Q-learning (Watkins & Dayan, 1992), a commonly used off-policy algorithm, uses the greedy policy
-µ(s) = arg maxa Q(s, a). We consider function approximators parameterized by θQ, which we
-optimize by minimizing the loss:
-
-              L(θQ) = Est∼ρβ,at∼β,rt∼E Q(st, at|θQ) − yt 2                    (4)
-
-where
-
-              yt = r(st, at) + γQ(st+1, µ(st+1)|θQ).                          (5)
-
-While yt is also dependent on θQ, this is typically ignored.
-
-The use of large, non-linear function approximators for learning value or action-value functions has
-often been avoided in the past since theoretical performance guarantees are impossible, and prac-
-tically learning tends to be unstable. Recently, (Mnih et al., 2013; 2015) adapted the Q-learning
-algorithm in order to make effective use of large neural networks as function approximators. Their
-algorithm was able to learn to play Atari games from pixels. In order to scale Q-learning they intro-
-duced two major changes: the use of a replay buffer, and a separate target network for calculating
-yt. We employ these in the context of DDPG and explain their implementation in the next section.
-
-3 ALGORITHM
-
-It is not possible to straightforwardly apply Q-learning to continuous action spaces, because in con-
-tinuous spaces ﬁnding the greedy policy requires an optimization of at at every timestep; this opti-
-mization is too slow to be practical with large, unconstrained function approximators and nontrivial
-action spaces. Instead, here we used an actor-critic approach based on the DPG algorithm (Silver
-et al., 2014).
-
-The DPG algorithm maintains a parameterized actor function µ(s|θµ) which speciﬁes the current
-policy by deterministically mapping states to a speciﬁc action. The critic Q(s, a) is learned using
-the Bellman equation as in Q-learning. The actor is updated by following the applying the chain rule
-to the expected return from the start distribution J with respect to the actor parameters:
-
-              ∇θµ J ≈ Est∼ρβ ∇θµ Q(s, a|θQ)|s=st,a=µ(st|θµ)                   (6)
-                      = Est∼ρβ ∇aQ(s, a|θQ)|s=st,a=µ(st)∇θµ µ(s|θµ)|s=st
-
-Silver et al. (2014) proved that this is the policy gradient, the gradient of the policy’s performance 2.
-
-As with Q learning, introducing non-linear function approximators means that convergence is no
-longer guaranteed. However, such approximators appear essential in order to learn and generalize
-on large state spaces. NFQCA (Hafner & Riedmiller, 2011), which uses the same update rules as
-DPG but with neural network function approximators, uses batch learning for stability, which is
-intractable for large networks. A minibatch version of NFQCA which does not reset the policy at
-each update, as would be required to scale to large networks, is equivalent to the original DPG,
-which we compare to here. Our contribution here is to provide modiﬁcations to DPG, inspired by
-the success of DQN, which allow it to use neural network function approximators to learn in large
-state and action spaces online. We refer to our algorithm as Deep DPG (DDPG, Algorithm 1).
-
-    2In practice, as in commonly done in policy gradient implementations, we ignored the discount in the state-
-visitation distribution ρβ.
-
-                                              3
-Published as a conference paper at ICLR 2016
-
-One challenge when using neural networks for reinforcement learning is that most optimization al-
-gorithms assume that the samples are independently and identically distributed. Obviously, when
-the samples are generated from exploring sequentially in an environment this assumption no longer
-holds. Additionally, to make efﬁcient use of hardware optimizations, it is essential to learn in mini-
-batches, rather than online.
-
-As in DQN, we used a replay buffer to address these issues. The replay buffer is a ﬁnite sized cache
-R. Transitions were sampled from the environment according to the exploration policy and the tuple
-(st, at, rt, st+1) was stored in the replay buffer. When the replay buffer was full the oldest samples
-were discarded. At each timestep the actor and critic are updated by sampling a minibatch uniformly
-from the buffer. Because DDPG is an off-policy algorithm, the replay buffer can be large, allowing
-the algorithm to beneﬁt from learning across a set of uncorrelated transitions.
-
-Directly implementing Q learning (equation 4) with neural networks proved to be unstable in many
-environments. Since the network Q(s, a|θQ) being updated is also used in calculating the target
-value (equation 5), the Q update is prone to divergence. Our solution is similar to the target network
-used in (Mnih et al., 2013) but modiﬁed for actor-critic and using “soft” target updates, rather than
-directly copying the weights. We create a copy of the actor and critic networks, Q (s, a|θQ ) and
-µ (s|θµ ) respectively, that are used for calculating the target values. The weights of these target
-networks are then updated by having them slowly track the learned networks: θ ← τ θ + (1 −
-τ )θ with τ 1. This means that the target values are constrained to change slowly, greatly
-improving the stability of learning. This simple change moves the relatively unstable problem of
-learning the action-value function closer to the case of supervised learning, a problem for which
-robust solutions exist. We found that having both a target µ and Q was required to have stable
-targets yi in order to consistently train the critic without divergence. This may slow learning, since
-the target network delays the propagation of value estimations. However, in practice we found this
-was greatly outweighed by the stability of learning.
-
-When learning from low dimensional feature vector observations, the different components of the
-observation may have different physical units (for example, positions versus velocities) and the
-ranges may vary across environments. This can make it difﬁcult for the network to learn effec-
-tively and may make it difﬁcult to ﬁnd hyper-parameters which generalise across environments with
-different scales of state values.
-
-One approach to this problem is to manually scale the features so they are in similar ranges across
-environments and units. We address this issue by adapting a recent technique from deep learning
-called batch normalization (Ioffe & Szegedy, 2015). This technique normalizes each dimension
-across the samples in a minibatch to have unit mean and variance. In addition, it maintains a run-
-ning average of the mean and variance to use for normalization during testing (in our case, during
-exploration or evaluation). In deep networks, it is used to minimize covariance shift during training,
-by ensuring that each layer receives whitened input. In the low-dimensional case, we used batch
-normalization on the state input and all layers of the µ network and all layers of the Q network prior
-to the action input (details of the networks are given in the supplementary material). With batch
-normalization, we were able to learn effectively across many different tasks with differing types of
-units, without needing to manually ensure the units were within a set range.
-
-A major challenge of learning in continuous action spaces is exploration. An advantage of off-
-
-policies algorithms such as DDPG is that we can treat the problem of exploration independently
-
-from the learning algorithm. We constructed an exploration policy µ by adding noise sampled from
-
-a noise process N to our actor policy
-
-                                       µ (st) = µ(st|θtµ) + N  (7)
-
-N can be chosen to suit the environment. As detailed in the supplementary materials we used
-
-an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) to generate temporally correlated
-
-exploration for exploration efﬁciency in physical control problems with inertia (similar use of auto-
-
-correlated noise was introduced in (Wawrzyn´ski, 2015)).
-
-4 RESULTS
-
-We constructed simulated physical environments of varying levels of difﬁculty to test our algorithm.
-This included classic reinforcement learning environments such as cartpole, as well as difﬁcult,
-
-                                              4
-Published as a conference paper at ICLR 2016
-
-Algorithm 1 DDPG algorithm
-
-Randomly initialize critic network Q(s, a|θQ) and actor µ(s|θµ) with weights θQ and θµ.
-
-Initialize target network Q and µ with weights θQ ← θQ, θµ ← θµ
-
-Initialize replay buffer R
-
-for episode = 1, M do
-
-Initialize a random process N for action exploration
-
-Receive initial observation state s1
-
-for t = 1, T do
-
-Select action at = µ(st|θµ) + Nt according to the current policy and exploration noise
-
-Execute action at and observe reward rt and observe new state st+1
-
-Store transition (st, at, rt, st+1) in R
-
-Sample a random minibatch of N transitions (si, ai, ri, si+1) from R
-
-Set yi = ri + γQ (si+1, µ (si+1|θµ )|θQ )             1  i(yi − Q(si, ai|θQ))2
-                                                      N
-Update  critic   by  minimizing     the  loss:  L  =
-
-Update the actor policy using the sampled policy gradient:
-
-                                 1       ∇aQ(s, a|θQ)|s=si,a=µ(si)∇θµ µ(s|θµ)|si
-                     ∇θµ J ≈ N
-                                    i
-
-      Update the target networks:        θQ ← τ θQ + (1 − τ )θQ
-                                         θµ ← τ θµ + (1 − τ )θµ
-   end for
-end for
-
-high dimensional tasks such as gripper, tasks involving contacts such as puck striking (canada)
-and locomotion tasks such as cheetah (Wawrzyn´ski, 2009). In all domains but cheetah the actions
-were torques applied to the actuated joints. These environments were simulated using MuJoCo
-(Todorov et al., 2012). Figure 1 shows renderings of some of the environments used in the task (the
-supplementary contains details of the environments and you can view some of the learned policies
-at https://goo.gl/J4PIAz).
-
-In all tasks, we ran experiments using both a low-dimensional state description (such as joint angles
-and positions) and high-dimensional renderings of the environment. As in DQN (Mnih et al., 2013;
-2015), in order to make the problems approximately fully observable in the high dimensional envi-
-ronment we used action repeats. For each timestep of the agent, we step the simulation 3 timesteps,
-repeating the agent’s action and rendering each time. Thus the observation reported to the agent
-contains 9 feature maps (the RGB of each of the 3 renderings) which allows the agent to infer veloc-
-ities using the differences between frames. The frames were downsampled to 64x64 pixels and the
-8-bit RGB values were converted to ﬂoating point scaled to [0, 1]. See supplementary information
-for details of our network structure and hyperparameters.
-
-We evaluated the policy periodically during training by testing it without exploration noise. Figure
-2 shows the performance curve for a selection of environments. We also report results with compo-
-nents of our algorithm (i.e. the target network or batch normalization) removed. In order to perform
-well across all tasks, both of these additions are necessary. In particular, learning without a target
-network, as in the original work with DPG, is very poor in many environments.
-
-Surprisingly, in some simpler tasks, learning policies from pixels is just as fast as learning using the
-low-dimensional state descriptor. This may be due to the action repeats making the problem simpler.
-It may also be that the convolutional layers provide an easily separable representation of state space,
-which is straightforward for the higher layers to learn on quickly.
-
-Table 1 summarizes DDPG’s performance across all of the environments (results are averaged over
-5 replicas). We normalized the scores using two baselines. The ﬁrst baseline is the mean return
-from a naive policy which samples actions from a uniform distribution over the valid action space.
-The second baseline is iLQG (Todorov & Li, 2005), a planning based solver with full access to the
-
-                                                           5
-Published as a conference paper at ICLR 2016
-
-underlying physical model and its derivatives. We normalize scores so that the naive policy has a
-mean score of 0 and iLQG has a mean score of 1. DDPG is able to learn good policies on many of
-the tasks, and in many cases some of the replicas learn policies which are superior to those found by
-iLQG, even when learning directly from pixels.
-
-It can be challenging to learn accurate value estimates. Q-learning, for example, is prone to over-
-estimating values (Hasselt, 2010). We examined DDPG’s estimates empirically by comparing the
-values estimated by Q after training with the true returns seen on test episodes. Figure 3 shows that
-in simple tasks DDPG estimates returns accurately without systematic biases. For harder tasks the
-Q estimates are worse, but DDPG is still able learn good policies.
-
-To demonstrate the generality of our approach we also include Torcs, a racing game where the
-actions are acceleration, braking and steering. Torcs has previously been used as a testbed in other
-policy learning approaches (Koutn´ık et al., 2014b). We used an identical network architecture and
-learning algorithm hyper-parameters to the physics tasks but altered the noise process for exploration
-because of the very different time scales involved. On both low-dimensional and from pixels, some
-replicas were able to learn reasonable policies that are able to complete a circuit around the track
-though other replicas failed to learn a sensible policy.
-
-Figure 1: Example screenshots of a sample of environments we attempt to solve with DDPG. In
-order from the left: the cartpole swing-up task, a reaching task, a gasp and move task, a puck-hitting
-task, a monoped balancing task, two locomotion tasks and Torcs (driving simulator). We tackle
-all tasks using both low-dimensional feature vector and high-dimensional pixel inputs. Detailed
-descriptions of the environments are provided in the supplementary. Movies of some of the learned
-policies are available at https://goo.gl/J4PIAz.
-
-                                      Cart           Pendulum Swing-up        Cartpole Swing-up             Fixed Reacher                 Monoped Balancing
-                   1                                                                                                            1
-                                                                                                      1
-                                                                                                                                0
-                                               1                           1
-
-Normalized Reward  0                           0                           0                          0                                    Moving Gripper
-                                     Gripper                   Blockworld              Puck Shooting                            1
-                                                                                                                    Cheetah
-                   1                           1                           1                          1
-                                               0                           0
-
-                   0                                                                                                            0
-
-                                                                                                      0
-
-                   0                        1     0  1                     0  1                          0                   1  0  1
-
-                      Million Steps
-
-Figure 2: Performance curves for a selection of domains using variants of DPG: original DPG
-algorithm (minibatch NFQCA) with batch normalization (light grey), with target network (dark
-grey), with target networks and batch normalization (green), with target networks from pixel-only
-inputs (blue). Target networks are crucial.
-
-5 RELATED WORK
-
-The original DPG paper evaluated the algorithm with toy problems using tile-coding and linear
-function approximators. It demonstrated data efﬁciency advantages for off-policy DPG over both
-on- and off-policy stochastic actor critic. It also solved one more challenging task in which a multi-
-jointed octopus arm had to strike a target with any part of the limb. However, that paper did not
-demonstrate scaling the approach to large, high-dimensional observation spaces as we have here.
-It has often been assumed that standard policy search methods such as those explored in the present
-work are simply too fragile to scale to difﬁcult problems (Levine et al., 2015). Standard policy search
-
-                                                           6
-Published as a conference paper at ICLR 2016
-
-             Pendulum                         Cartpole            Cheetah
-
-Estimated Q
-
-                                                                        Return
-
-Figure 3: Density plot showing estimated Q values versus observed returns sampled from test
-episodes on 5 replicas. In simple domains such as pendulum and cartpole the Q values are quite
-accurate. In more complex tasks, the Q estimates are less accurate, but can still be used to learn
-competent policies. Dotted line indicates unity, units are arbitrary.
-
-Table 1: Performance after training across all environments for at most 2.5 million steps. We report
-both the average and best observed (across 5 runs). All scores, except Torcs, are normalized so
-that a random agent receives 0 and a planning algorithm 1; for Torcs we present the raw reward
-score. We include results from the DDPG algorithn in the low-dimensional (lowd) version of the
-environment and high-dimensional (pix). For comparision we also include results from the original
-DPG algorithm with a replay buffer and batch normalization (cntrl).
-
-       environment       Rav,lowd  Rbest,lowd           Rav,pix   Rbest,pix  Rav,cntrl  Rbest,cntrl
-       blockworld1         1.156     1.511                0.466     1.299     -0.080       1.260
-      blockworld3da        0.340     0.705                0.889     2.225     -0.139       0.658
-                           0.303     1.735                0.176     0.688      0.125       1.157
-           canada          0.400     0.978               -0.285     0.119     -0.045       0.701
-         canada2d          0.938     1.336                1.096     1.258      0.343       1.216
-                           0.844     1.115                0.482     1.138      0.244       0.755
-             cart          0.951     1.000                0.335     0.996     -0.468       0.528
-          cartpole         0.549     0.900                0.188     0.323      0.197       0.572
-     cartpoleBalance       0.272     0.719                0.195     0.642      0.143       0.701
- cartpoleParallelDouble    0.736     0.946                0.412     0.427      0.583       0.942
-  cartpoleSerialDouble     0.903     1.206                0.457     0.792     -0.008       0.425
-   cartpoleSerialTriple    0.849     1.021                0.693     0.981      0.259       0.927
-          cheetah          0.924     0.996                0.872     0.943      0.290       0.995
-       ﬁxedReacher         0.954     1.000                0.827     0.995      0.620       0.999
-   ﬁxedReacherDouble       0.655     0.972                0.406     0.790      0.461       0.816
-   ﬁxedReacherSingle       0.618     0.937                0.082     0.791      0.557       0.808
-           gripper         1.311     1.990                1.204     1.431     -0.031       1.411
-      gripperRandom        0.676     0.936                0.112     0.924      0.078       0.917
-       hardCheetah         0.416     0.722                0.234     0.672      0.198       0.618
-           hopper          0.474     0.936                0.480     0.644      0.416       0.805
-                           0.946     1.021                0.663     1.055      0.099       0.951
-             hyq           0.720     0.987                0.194     0.878      0.231       0.953
-      movingGripper        0.585     0.943                0.453     0.922      0.204       0.631
-                           0.467     0.739                0.374     0.735     -0.046       0.158
-         pendulum          0.981     1.102                1.000     1.083      1.010       1.083
-           reacher         0.705     1.573                0.944     1.476      0.393       1.397
-                                                        -401.911             -911.034
- reacher3daFixedTarget   -393.385  1840.036                       1876.284              1961.600
-reacher3daRandomTarget
-
-       reacherSingle
-         walker2d
-            torcs
-
-is thought to be difﬁcult because it deals simultaneously with complex environmental dynamics and
-a complex policy. Indeed, most past work with actor-critic and policy optimization approaches have
-had difﬁculty scaling up to more challenging problems (Deisenroth et al., 2013). Typically, this
-is due to instability in learning wherein progress on a problem is either destroyed by subsequent
-learning updates, or else learning is too slow to be practical.
-
-Recent work with model-free policy search has demonstrated that it may not be as fragile as previ-
-ously supposed. Wawrzyn´ski (2009); Wawrzyn´ski & Tanwani (2013) has trained stochastic policies
-
-                                                           7
-Published as a conference paper at ICLR 2016
-
-in an actor-critic framework with a replay buffer. Concurrent with our work, Balduzzi & Ghifary
-(2015) extended the DPG algorithm with a “deviator” network which explicitly learns ∂Q/∂a. How-
-ever, they only train on two low-dimensional domains. Heess et al. (2015) introduced SVG(0) which
-also uses a Q-critic but learns a stochastic policy. DPG can be considered the deterministic limit of
-SVG(0). The techniques we described here for scaling DPG are also applicable to stochastic policies
-by using the reparametrization trick (Heess et al., 2015; Schulman et al., 2015a).
-
-Another approach, trust region policy optimization (TRPO) (Schulman et al., 2015b), directly con-
-structs stochastic neural network policies without decomposing problems into optimal control and
-supervised phases. This method produces near monotonic improvements in return by making care-
-fully chosen updates to the policy parameters, constraining updates to prevent the new policy from
-diverging too far from the existing policy. This approach does not require learning an action-value
-function, and (perhaps as a result) appears to be signiﬁcantly less data efﬁcient.
-
-To combat the challenges of the actor-critic approach, recent work with guided policy search (GPS)
-algorithms (e.g., (Levine et al., 2015)) decomposes the problem into three phases that are rela-
-tively easy to solve: ﬁrst, it uses full-state observations to create locally-linear approximations of
-the dynamics around one or more nominal trajectories, and then uses optimal control to ﬁnd the
-locally-linear optimal policy along these trajectories; ﬁnally, it uses supervised learning to train a
-complex, non-linear policy (e.g. a deep neural network) to reproduce the state-to-action mapping of
-the optimized trajectories.
-
-This approach has several beneﬁts, including data efﬁciency, and has been applied successfully to
-a variety of real-world robotic manipulation tasks using vision. In these tasks GPS uses a similar
-convolutional policy network to ours with 2 notable exceptions: 1. it uses a spatial softmax to reduce
-the dimensionality of visual features into a single (x, y) coordinate for each feature map, and 2. the
-policy also receives direct low-dimensional state information about the conﬁguration of the robot at
-the ﬁrst fully connected layer in the network. Both likely increase the power and data efﬁciency of
-the algorithm and could easily be exploited within the DDPG framework.
-
-PILCO (Deisenroth & Rasmussen, 2011) uses Gaussian processes to learn a non-parametric, proba-
-bilistic model of the dynamics. Using this learned model, PILCO calculates analytic policy gradients
-and achieves impressive data efﬁciency in a number of control problems. However, due to the high
-computational demand, PILCO is “impractical for high-dimensional problems” (Wahlstro¨m et al.,
-2015). It seems that deep function approximators are the most promising approach for scaling rein-
-forcement learning to large, high-dimensional domains.
-
-Wahlstro¨m et al. (2015) used a deep dynamical model network along with model predictive control
-to solve the pendulum swing-up task from pixel input. They trained a differentiable forward model
-and encoded the goal state into the learned latent space. They use model-predictive control over the
-learned model to ﬁnd a policy for reaching the target. However, this approach is only applicable to
-domains with goal states that can be demonstrated to the algorithm.
-
-Recently, evolutionary approaches have been used to learn competitive policies for Torcs from pixels
-using compressed weight parametrizations (Koutn´ık et al., 2014a) or unsupervised learning (Koutn´ık
-et al., 2014b) to reduce the dimensionality of the evolved weights. It is unclear how well these
-approaches generalize to other problems.
-
-6 CONCLUSION
-
-The work combines insights from recent advances in deep learning and reinforcement learning, re-
-sulting in an algorithm that robustly solves challenging problems across a variety of domains with
-continuous action spaces, even when using raw pixels for observations. As with most reinforcement
-learning algorithms, the use of non-linear function approximators nulliﬁes any convergence guar-
-antees; however, our experimental results demonstrate that stable learning without the need for any
-modiﬁcations between environments.
-
-Interestingly, all of our experiments used substantially fewer steps of experience than was used by
-DQN learning to ﬁnd solutions in the Atari domain. Nearly all of the problems we looked at were
-solved within 2.5 million steps of experience (and usually far fewer), a factor of 20 fewer steps than
-
-                                                           8
-Published as a conference paper at ICLR 2016
-
-DQN requires for good Atari solutions. This suggests that, given more simulation time, DDPG may
-solve even more difﬁcult problems than those considered here.
-A few limitations to our approach remain. Most notably, as with most model-free reinforcement
-approaches, DDPG requires a large number of training episodes to ﬁnd solutions. However, we
-believe that a robust model-free approach may be an important component of larger systems which
-may attack these limitations (Gla¨scher et al., 2010).
-
-REFERENCES
-
-Balduzzi, David and Ghifary, Muhammad. Compatible value gradients for reinforcement learning
-   of continuous deep policies. arXiv preprint arXiv:1509.03005, 2015.
-
-Deisenroth, Marc and Rasmussen, Carl E. Pilco: A model-based and data-efﬁcient approach to
-   policy search. In Proceedings of the 28th International Conference on machine learning (ICML-
-   11), pp. 465–472, 2011.
-
-Deisenroth, Marc Peter, Neumann, Gerhard, Peters, Jan, et al. A survey on policy search for robotics.
-   Foundations and Trends in Robotics, 2(1-2):1–142, 2013.
-
-Gla¨scher, Jan, Daw, Nathaniel, Dayan, Peter, and O’Doherty, John P. States versus rewards: dis-
-   sociable neural prediction error signals underlying model-based and model-free reinforcement
-   learning. Neuron, 66(4):585–595, 2010.
-
-Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer networks. In Proceed-
-   ings of the 14th International Conference on Artiﬁcial Intelligence and Statistics. JMLR W&CP
-   Volume, volume 15, pp. 315–323, 2011.
-
-Hafner, Roland and Riedmiller, Martin. Reinforcement learning in feedback control. Machine
-   learning, 84(1-2):137–169, 2011.
-
-Hasselt, Hado V. Double q-learning. In Advances in Neural Information Processing Systems, pp.
-   2613–2621, 2010.
-
-Heess, N., Hunt, J. J, Lillicrap, T. P, and Silver, D. Memory-based control with recurrent neural
-   networks. NIPS Deep Reinforcement Learning Workshop (arXiv:1512.04455), 2015.
-
-Heess, Nicolas, Wayne, Gregory, Silver, David, Lillicrap, Tim, Erez, Tom, and Tassa, Yuval. Learn-
-   ing continuous control policies by stochastic value gradients. In Advances in Neural Information
-   Processing Systems, pp. 2926–2934, 2015.
-
-Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by
-   reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
-
-Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint
-   arXiv:1412.6980, 2014.
-
-Koutn´ık, Jan, Schmidhuber, Ju¨rgen, and Gomez, Faustino. Evolving deep unsupervised convolu-
-   tional networks for vision-based reinforcement learning. In Proceedings of the 2014 conference
-   on Genetic and evolutionary computation, pp. 541–548. ACM, 2014a.
-
-Koutn´ık, Jan, Schmidhuber, Ju¨rgen, and Gomez, Faustino. Online evolution of deep convolutional
-   network for vision-based reinforcement learning. In From Animals to Animats 13, pp. 260–269.
-   Springer, 2014b.
-
-Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo-
-   lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
-   2012.
-
-Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep
-   visuomotor policies. arXiv preprint arXiv:1504.00702, 2015.
-
-                                                           9
-Published as a conference paper at ICLR 2016
-
-Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves, Alex, Antonoglou, Ioannis, Wier-
-   stra, Daan, and Riedmiller, Martin. Playing atari with deep reinforcement learning. arXiv preprint
-   arXiv:1312.5602, 2013.
-
-Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare,
-   Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-
-   level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.
-
-Prokhorov, Danil V, Wunsch, Donald C, et al. Adaptive critic designs. Neural Networks, IEEE
-   Transactions on, 8(5):997–1007, 1997.
-
-Schulman, John, Heess, Nicolas, Weber, Theophane, and Abbeel, Pieter. Gradient estimation using
-   stochastic computation graphs. In Advances in Neural Information Processing Systems, pp. 3510–
-   3522, 2015a.
-
-Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael I, and Abbeel, Pieter. Trust region
-   policy optimization. arXiv preprint arXiv:1502.05477, 2015b.
-
-Silver, David, Lever, Guy, Heess, Nicolas, Degris, Thomas, Wierstra, Daan, and Riedmiller, Martin.
-   Deterministic policy gradient algorithms. In ICML, 2014.
-
-Tassa, Yuval, Erez, Tom, and Todorov, Emanuel. Synthesis and stabilization of complex behaviors
-   through online trajectory optimization. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
-   International Conference on, pp. 4906–4913. IEEE, 2012.
-
-Todorov, Emanuel and Li, Weiwei. A generalized iterative lqg method for locally-optimal feed-
-   back control of constrained nonlinear stochastic systems. In American Control Conference, 2005.
-   Proceedings of the 2005, pp. 300–306. IEEE, 2005.
-
-Todorov, Emanuel, Erez, Tom, and Tassa, Yuval. Mujoco: A physics engine for model-based control.
-   In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026–
-   5033. IEEE, 2012.
-
-Uhlenbeck, George E and Ornstein, Leonard S. On the theory of the brownian motion. Physical
-   review, 36(5):823, 1930.
-
-Wahlstro¨m, Niklas, Scho¨n, Thomas B, and Deisenroth, Marc Peter. From pixels to torques: Policy
-   learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.
-
-Watkins, Christopher JCH and Dayan, Peter. Q-learning. Machine learning, 8(3-4):279–292, 1992.
-Wawrzyn´ski, Paweł. Real-time reinforcement learning by sequential actor–critics and experience
-
-   replay. Neural Networks, 22(10):1484–1497, 2009.
-Wawrzyn´ski, Paweł. Control policy with autocorrelated noise in reinforcement learning for robotics.
-
-   International Journal of Machine Learning and Computing, 5:91–95, 2015.
-Wawrzyn´ski, Paweł and Tanwani, Ajay Kumar. Autonomous reinforcement learning with experience
-
-   replay. Neural Networks, 41:156–167, 2013.
-
-                                                          10
-Published as a conference paper at ICLR 2016
-
-Supplementary Information: Continuous control with
-                deep reinforcement learning
-
-7 EXPERIMENT DETAILS
-
-We used Adam (Kingma & Ba, 2014) for learning the neural network parameters with a learning
-rate of 10−4 and 10−3 for the actor and critic respectively. For Q we included L2 weight decay of
-10−2 and used a discount factor of γ = 0.99. For the soft target updates we used τ = 0.001. The
-
-neural networks used the rectiﬁed non-linearity (Glorot et al., 2011) for all hidden layers. The ﬁnal
-
-output layer of the actor was a tanh layer, to bound the actions. The low-dimensional networks
-
-had 2 hidden layers with 400 and 300 units respectively (≈ 130,000 parameters). Actions were not
-
-included until the 2nd hidden layer of Q. When learning from pixels we used 3 convolutional layers
-
-(no pooling) with 32 ﬁlters at each layer. This was followed by two fully connected layers with
-
-200 units (≈ 430,000 parameters). The ﬁnal layer weights and biases of both the actor and critic
-were initialized from a uniform distribution [−3 × 10−3, 3 × 10−3] and [3 × 10−4, 3 × 10−4] for the
-
-low dimensional and pixel cases respectively. This was to ensure the initial outputs for the policy
-
-and value estimates were near zero. The other layers were initialized from uniform distributions
-
-[−  √1   ,  √1   ]  where  f  is  the  fan-in  of  the  layer.  The  actions  were  not  included  until  the  fully-connected
-      f       f
-
-layers. We trained with minibatch sizes of 64 for the low dimensional problems and 16 on pixels.
-
-We used a replay buffer size of 106.
-
-For the exploration noise process we used temporally correlated noise in order to explore well in
-physical environments that have momentum. We used an Ornstein-Uhlenbeck process (Uhlenbeck
-& Ornstein, 1930) with θ = 0.15 and σ = 0.2. The Ornstein-Uhlenbeck process models the velocity
-of a Brownian particle with friction, which results in temporally correlated values centered around
-0.
-
-8 PLANNING ALGORITHM
-
-Our planner is implemented as a model-predictive controller (Tassa et al., 2012): at every time step
-we run a single iteration of trajectory optimization (using iLQG, (Todorov & Li, 2005)), starting
-from the true state of the system. Every single trajectory optimization is planned over a horizon
-between 250ms and 600ms, and this planning horizon recedes as the simulation of the world unfolds,
-as is the case in model-predictive control.
-
-The iLQG iteration begins with an initial rollout of the previous policy, which determines the nom-
-inal trajectory. We use repeated samples of simulated dynamics to approximate a linear expansion
-of the dynamics around every step of the trajectory, as well as a quadratic expansion of the cost
-function. We use this sequence of locally-linear-quadratic models to integrate the value function
-backwards in time along the nominal trajectory. This back-pass results in a putative modiﬁcation to
-the action sequence that will decrease the total cost. We perform a derivative-free line-search over
-this direction in the space of action sequences by integrating the dynamics forward (the forward-
-pass), and choose the best trajectory. We store this action sequence in order to warm-start the next
-iLQG iteration, and execute the ﬁrst action in the simulator. This results in a new state, which is
-used as the initial state in the next iteration of trajectory optimization.
-
-9 ENVIRONMENT DETAILS
-
-9.1 TORCS ENVIRONMENT
-
-For the Torcs environment we used a reward function which provides a positive reward at each step
-for the velocity of the car projected along the track direction and a penalty of −1 for collisions.
-Episodes were terminated if progress was not made along the track after 500 frames.
-
-                                                          11
-Published as a conference paper at ICLR 2016
-
-9.2 MUJOCO ENVIRONMENTS
-
-For physical control tasks we used reward functions which provide feedback at every step. In all
-tasks, the reward contained a small action cost. For all tasks that have a static goal state (e.g.
-pendulum swingup and reaching) we provide a smoothly varying reward based on distance to a goal
-state, and in some cases an additional positive reward when within a small radius of the target state.
-For grasping and manipulation tasks we used a reward with a term which encourages movement
-towards the payload and a second component which encourages moving the payload to the target. In
-locomotion tasks we reward forward action and penalize hard impacts to encourage smooth rather
-than hopping gaits (Schulman et al., 2015b). In addition, we used a negative reward and early
-termination for falls which were determined by simple threshholds on the height and torso angle (in
-the case of walker2d).
-
-Table 2 states the dimensionality of the problems and below is a summary of all the physics envi-
-ronments.
-
-             task name                        dim(s)  dim(a)  dim(o)
-             blockworld1
-             blockworld3da                      18       5       43
-             canada                             31       9      102
-             canada2d                           22       7       62
-             cart                               14       3       29
-             cartpole                            2       1
-             cartpoleBalance                     4       1       3
-             cartpoleParallelDouble              4       1       14
-             cartpoleParallelTriple              6       1       14
-             cartpoleSerialDouble                8       1       16
-             cartpoleSerialTriple                6       1       23
-             cheetah                             8       1       14
-             ﬁxedReacher                        18       6       23
-             ﬁxedReacherDouble                  10       3       17
-             ﬁxedReacherSingle                   8       2       23
-             gripper                             6       1       18
-             gripperRandom                      18       5       13
-             hardCheetah                        18       5       43
-             hardCheetahNice                    18       6       43
-             hopper                             18       6       17
-             hyq                                14       4       17
-             hyqKick                            37       12      14
-             movingGripper                      37       12      37
-             movingGripperRandom                22       7       37
-             pendulum                           22       7       49
-             reacher                             2       1       49
-             reacher3daFixedTarget              10       3       3
-             reacher3daRandomTarget             20       7       23
-             reacherDouble                      20       7       61
-             reacherObstacle                     6       1       61
-             reacherSingle                      18       5       13
-             walker2d                            6       1       38
-                                                18       6       13
-                                                                 41
-
-Table 2: Dimensionality of the MuJoCo tasks: the dimensionality of the underlying physics model
-dim(s), number of action dimensions dim(a) and observation dimensions dim(o).
-
-task name    Brief Description
-blockworld1  Agent is required to use an arm with gripper constrained to the 2D plane
-             to grab a falling block and lift it against gravity to a ﬁxed target position.
-
-                                  12
-Published as a conference paper at ICLR 2016
-
-blockworld3da           Agent is required to use a human-like arm with 7-DOF and a simple
-canada                  gripper to grab a block and lift it against gravity to a ﬁxed target posi-
-canada2d                tion.
-cart
-cartpole                Agent is required to use a 7-DOF arm with hockey-stick like appendage
-cartpoleBalance         to hit a ball to a target.
-cartpoleParallelDouble
-cartpoleSerialDouble    Agent is required to use an arm with hockey-stick like appendage to hit
-cartpoleSerialTriple    a ball initialzed to a random start location to a random target location.
-cheetah
-                        Agent must move a simple mass to rest at 0. The mass begins each trial
-ﬁxedReacher             in random positions and with random velocities.
-ﬁxedReacherDouble
-ﬁxedReacherSingle       The classic cart-pole swing-up task. Agent must balance a pole at-
-gripper                 tached to a cart by applying forces to the cart alone. The pole starts
-gripperRandom           each episode hanging upside-down.
-hardCheetah
-                        The classic cart-pole balance task. Agent must balance a pole attached
-hopper                  to a cart by applying forces to the cart alone. The pole starts in the
-hyq                     upright positions at the beginning of each episode.
-
-                        Variant on the classic cart-pole. Two poles, both attached to the cart,
-                        should be kept upright as much as possible.
-
-                        Variant on the classic cart-pole. Two poles, one attached to the cart and
-                        the second attached to the end of the ﬁrst, should be kept upright as
-                        much as possible.
-
-                        Variant on the classic cart-pole. Three poles, one attached to the cart,
-                        the second attached to the end of the ﬁrst, and the third attached to the
-                        end of the second, should be kept upright as much as possible.
-
-                        The agent should move forward as quickly as possible with a cheetah-
-                        like body that is constrained to the plane. This environment is based
-                        very closely on the one introduced by Wawrzyn´ski (2009); Wawrzyn´ski
-                        & Tanwani (2013).
-
-                        Agent is required to move a 3-DOF arm to a ﬁxed target position.
-
-                        Agent is required to move a 2-DOF arm to a ﬁxed target position.
-
-                        Agent is required to move a simple 1-DOF arm to a ﬁxed target position.
-
-                        Agent must use an arm with gripper appendage to grasp an object and
-                        manuver the object to a ﬁxed target.
-
-                        The same task as gripper except that the arm object and target posi-
-                        tion are initialized in random locations.
-
-                        The agent should move forward as quickly as possible with a cheetah-
-                        like body that is constrained to the plane. This environment is based
-                        very closely on the one introduced by Wawrzyn´ski (2009); Wawrzyn´ski
-                        & Tanwani (2013), but has been made much more difﬁcult by removing
-                        the stabalizing joint stiffness from the model.
-
-                        Agent must balance a multiple degree of freedom monoped to keep it
-                        from falling.
-
-                        Agent is required to keep a quadroped model based on the hyq robot
-                        from falling.
-
-                                              13
-Published as a conference paper at ICLR 2016
-
-movingGripper          Agent must use an arm with gripper attached to a moveable platform to
-                       grasp an object and move it to a ﬁxed target.
-
-movingGripperRandom The same as the movingGripper environment except that the object po-
-                                  sition, target position, and arm state are initialized randomly.
-
-pendulum               The classic pendulum swing-up problem. The pendulum should be
-                       brought to the upright position and balanced. Torque limits prevent the
-                       agent from swinging the pendulum up directly.
-
-reacher3daFixedTarget  Agent is required to move a 7-DOF human-like arm to a ﬁxed target
-                       position.
-
-reacher3daRandomTarget Agent is required to move a 7-DOF human-like arm from random start-
-                                  ing locations to random target positions.
-
-reacher                Agent is required to move a 3-DOF arm from random starting locations
-                       to random target positions.
-
-reacherSingle          Agent is required to move a simple 1-DOF arm from random starting
-                       locations to random target positions.
-
-reacherObstacle        Agent is required to move a 5-DOF arm around an obstacle to a ran-
-                       domized target position.
-
-walker2d               Agent should move forward as quickly as possible with a bipedal walker
-                       constrained to the plane without falling down or pitching the torso too
-                       far forward or backward.
-
-                                              14
-
diff --git a/reference papers/PPO_paper.pdf b/reference papers/PPO_paper.pdf
deleted file mode 100644
index 920123b..0000000
--- a/reference papers/PPO_paper.pdf	
+++ /dev/null
@@ -1,718 +0,0 @@
-arXiv:1707.06347v2 [cs.LG] 28 Aug 2017                 Proximal Policy Optimization Algorithms
-
-                                                 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov
-                                                                                              OpenAI
-
-                                                             {joschu, filip, prafulla, alec, oleg}@openai.com
-
-                                                                                                        Abstract
-
-                                                    We propose a new family of policy gradient methods for reinforcement learning, which al-
-                                                ternate between sampling data through interaction with the environment, and optimizing a
-                                                “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gra-
-                                                dient methods perform one gradient update per data sample, we propose a novel objective
-                                                function that enables multiple epochs of minibatch updates. The new methods, which we call
-                                                proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimiza-
-                                                tion (TRPO), but they are much simpler to implement, more general, and have better sample
-                                                complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-
-                                                ing simulated robotic locomotion and Atari game playing, and we show that PPO outperforms
-                                                other online policy gradient methods, and overall strikes a favorable balance between sample
-                                                complexity, simplicity, and wall-time.
-
-                                        1 Introduction
-
-                                        In recent years, several diﬀerent approaches have been proposed for reinforcement learning with
-                                        neural network function approximators. The leading contenders are deep Q-learning [Mni+15],
-                                        “vanilla” policy gradient methods [Mni+16], and trust region / natural policy gradient methods
-                                        [Sch+15b]. However, there is room for improvement in developing a method that is scalable (to
-                                        large models and parallel implementations), data eﬃcient, and robust (i.e., successful on a variety
-                                        of problems without hyperparameter tuning). Q-learning (with function approximation) fails on
-                                        many simple problems1 and is poorly understood, vanilla policy gradient methods have poor data
-                                        eﬃency and robustness; and trust region policy optimization (TRPO) is relatively complicated,
-                                        and is not compatible with architectures that include noise (such as dropout) or parameter sharing
-                                        (between the policy and value function, or with auxiliary tasks).
-
-                                            This paper seeks to improve the current state of aﬀairs by introducing an algorithm that attains
-                                        the data eﬃciency and reliable performance of TRPO, while using only ﬁrst-order optimization.
-                                        We propose a novel objective with clipped probability ratios, which forms a pessimistic estimate
-                                        (i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between
-                                        sampling data from the policy and performing several epochs of optimization on the sampled data.
-
-                                            Our experiments compare the performance of various diﬀerent versions of the surrogate objec-
-                                        tive, and ﬁnd that the version with the clipped probability ratios performs best. We also compare
-                                        PPO to several previous algorithms from the literature. On continuous control tasks, it performs
-                                        better than the algorithms we compare against. On Atari, it performs signiﬁcantly better (in terms
-                                        of sample complexity) than A2C and similarly to ACER though it is much simpler.
-
-                                            1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete
-                                        action spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in
-                                        OpenAI Gym [Bro+16] and described by Duan et al. [Dua+16].
-
-                                                                                                        1
-2 Background: Policy Optimization
-
-2.1 Policy Gradient Methods
-
-Policy gradient methods work by computing an estimator of the policy gradient and plugging it
-into a stochastic gradient ascent algorithm. The most commonly used gradient estimator has the
-form
-
-                          gˆ = Eˆt ∇θ log πθ(at | st)Aˆt                                     (1)
-
-where πθ is a stochastic policy and Aˆt is an estimator of the advantage function at timestep t.
-Here, the expectation Eˆt[. . .] indicates the empirical average over a ﬁnite batch of samples, in an
-algorithm that alternates between sampling and optimization. Implementations that use automatic
-
-diﬀerentiation software work by constructing an objective function whose gradient is the policy
-
-gradient estimator; the estimator gˆ is obtained by diﬀerentiating the objective
-
-              LP G(θ) = Eˆt log πθ(at | st)Aˆt .                                             (2)
-
-While it is appealing to perform multiple steps of optimization on this loss LP G using the same
-trajectory, doing so is not well-justiﬁed, and empirically it often leads to destructively large policy
-updates (see Section 6.1; results are not shown but were similar or worse than the “no clipping or
-penalty” setting).
-
-2.2 Trust Region Methods
-
-In TRPO [Sch+15b], an objective function (the “surrogate” objective) is maximized subject to a
-constraint on the size of the policy update. Speciﬁcally,
-
-maximize                     Eˆ t   πθ(at |       st)   )  Aˆt                               (3)
-                                   πθold (at      | st
-       θ
-
-subject to Eˆt[KL[πθold(· | st), πθ(· | st)]] ≤ δ.                                           (4)
-
-Here, θold is the vector of policy parameters before the update. This problem can eﬃciently be
-approximately solved using the conjugate gradient algorithm, after making a linear approximation
-to the objective and a quadratic approximation to the constraint.
-
-    The theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,
-solving the unconstrained optimization problem
-
-maximize Eˆt   πθ(at |             st)    Aˆt  −  β  KL[πθold   (·  |  st),  πθ (·  |  st)]  (5)
-              πθold (at            | st)
-       θ
-
-for some coeﬃcient β. This follows from the fact that a certain surrogate objective (which computes
-the max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the
-performance of the policy π. TRPO uses a hard constraint rather than a penalty because it is hard
-to choose a single value of β that performs well across diﬀerent problems—or even within a single
-problem, where the the characteristics change over the course of learning. Hence, to achieve our goal
-of a ﬁrst-order algorithm that emulates the monotonic improvement of TRPO, experiments show
-that it is not suﬃcient to simply choose a ﬁxed penalty coeﬃcient β and optimize the penalized
-objective Equation (5) with SGD; additional modiﬁcations are required.
-
-                                               2
-3 Clipped Surrogate Objective
-
-Let  rt(θ)  denote  the  probability  ratio  rt(θ)  =       πθ(at |   st)   )  ,  so  r(θold)  =  1.  TRPO maximizes a
-                                                           πθold (at  | st
-
-“surrogate” objective
-
-                         LCP I (θ) = Eˆt         πθ(at |   st)   )  Aˆt  = Eˆt rt(θ)Aˆt .                (6)
-                                                πθold (at  | st
-
-The superscript CP I refers to conservative policy iteration [KL02], where this objective was pro-
-posed. Without a constraint, maximization of LCP I would lead to an excessively large policy
-
-update; hence, we now consider how to modify the objective, to penalize changes to the policy that
-
-move rt(θ) away from 1.
-    The main objective we propose is the following:
-
-                         LCLIP (θ) = Eˆt min(rt(θ)Aˆt, clip(rt(θ), 1 − , 1 + )Aˆt)                       (7)
-
-where epsilon is a hyperparameter, say, = 0.2. The motivation for this objective is as follows. The
-ﬁrst term inside the min is LCP I . The second term, clip(rt(θ), 1 − , 1 + )Aˆt, modiﬁes the surrogate
-objective by clipping the probability ratio, which removes the incentive for moving rt outside of the
-interval [1 − , 1 + ]. Finally, we take the minimum of the clipped and unclipped objective, so the
-
-ﬁnal objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective. With this
-
-scheme, we only ignore the change in probability ratio when it would make the objective improve,
-and we include it when it makes the objective worse. Note that LCLIP (θ) = LCP I (θ) to ﬁrst order
-
-around θold (i.e., where r = 1), however, they become diﬀerent as θ moves away from θold. Figure 1
-plots a single term (i.e., a single t) in LCLIP ; note that the probability ratio r is clipped at 1 −
-
-or 1 + depending on whether the advantage is positive or negative.
-
-               LCLIP A > 0                                                            A<0
-
-                                                                         0            1− 1            r
-
-                                             r                              LC LI P
-
-            0            1 1+
-
-Figure 1: Plots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of
-
-the probability ratio r, for positive advantages (left) and negative advantages (right). The red circle on each
-plot shows the starting point for the optimization, i.e., r = 1. Note that LCLIP sums many of these terms.
-
-    Figure 2 provides another source of intuition about the surrogate objective LCLIP . It shows how
-several objectives vary as we interpolate along the policy update direction, obtained by proximal
-policy optimization (the algorithm we will introduce shortly) on a continuous control problem. We
-can see that LCLIP is a lower bound on LCP I , with a penalty for having too large of a policy
-update.
-
-                                                       3
-0.12                                                         Et[KLt]
-                                                             LCPI = Et[rtAt]
-0.10                                                         Et[clip(rt, 1 , 1 + )At]
-                                                             LCLIP = Et[min(rtAt, clip(rt, 1
-0.08                                                                                              , 1 + )At)]
-
-0.06
-
-0.04
-
-0.02
-
-0.00
-
-0.02  0Linear interpolati1on factor
-
-Figure 2: Surrogate objectives, as we interpolate between the initial policy parameter θold, and the updated
-policy parameter, which we compute after one iteration of PPO. The updated policy has a KL divergence of
-about 0.02 from the initial policy, and this is the point at which LCLIP is maximal. This plot corresponds
-
-to the ﬁrst policy update on the Hopper-v1 problem, using hyperparameters provided in Section 6.1.
-
-4 Adaptive KL Penalty Coeﬃcient
-
-Another approach, which can be used as an alternative to the clipped surrogate objective, or in
-addition to it, is to use a penalty on KL divergence, and to adapt the penalty coeﬃcient so that we
-achieve some target value of the KL divergence dtarg each policy update. In our experiments, we
-found that the KL penalty performed worse than the clipped surrogate objective, however, we’ve
-included it here because it’s an important baseline.
-
-    In the simplest instantiation of this algorithm, we perform the following steps in each policy
-update:
-
-• Using several epochs of minibatch SGD, optimize the KL-penalized objective
-
-      LKLP EN (θ) = Eˆt   πθ(at |    st)             Aˆt  −  β  KL[πθold  (·  |  st),  πθ  (·  |  st)]         (8)
-                         πθold (at   | st)
-
-• Compute d = Eˆt[KL[πθold(· | st), πθ(· | st)]]
-
-      – If d < dtarg/1.5, β ← β/2
-      – If d > dtarg × 1.5, β ← β × 2
-
-The updated β is used for the next policy update. With this scheme, we occasionally see policy
-updates where the KL divergence is signiﬁcantly diﬀerent from dtarg, however, these are rare, and
-β quickly adjusts. The parameters 1.5 and 2 above are chosen heuristically, but the algorithm is
-not very sensitive to them. The initial value of β is a another hyperparameter but is not important
-in practice because the algorithm quickly adjusts it.
-
-5 Algorithm
-
-The surrogate losses from the previous sections can be computed and diﬀerentiated with a minor
-change to a typical policy gradient implementation. For implementations that use automatic dif-
-ferentation, one simply constructs the loss LCLIP or LKLP EN instead of LP G, and one performs
-multiple steps of stochastic gradient ascent on this objective.
-
-    Most techniques for computing variance-reduced advantage-function estimators make use a
-learned state-value function V (s); for example, generalized advantage estimation [Sch+15a], or the
-
-                                                  4
-ﬁnite-horizon estimators in [Mni+16]. If using a neural network architecture that shares parameters
-between the policy and value function, we must use a loss function that combines the policy
-surrogate and a value function error term. This objective can further be augmented by adding
-an entropy bonus to ensure suﬃcient exploration, as suggested in past work [Wil92; Mni+16].
-Combining these terms, we obtain the following objective, which is (approximately) maximized
-each iteration:
-
-LCt LIP +V F +S(θ) = Eˆt LCt LIP (θ) − c1LtV F (θ) + c2S[πθ](st) ,                                    (9)
-
-where c1, c2 are coeﬃcients,  and  S  denotes  an  entropy  bonus,  and  LVt F  is  a  squared-error  loss
-(Vθ(st) − Vttarg)2.
-
-One style of policy gradient implementation, popularized in [Mni+16] and well-suited for use
-
-with recurrent neural networks, runs the policy for T timesteps (where T is much less than the
-
-episode length), and uses the collected samples for an update. This style requires an advantage
-
-estimator that does not look beyond timestep T . The estimator used by [Mni+16] is
-
-Aˆt = −V (st) + rt + γrt+1 + · · · + γT −t+1rT −1 + γT −tV (sT )                                      (10)
-
-where t speciﬁes the time index in [0, T ], within a given length-T trajectory segment. Generalizing
-this choice, we can use a truncated version of generalized advantage estimation, which reduces to
-Equation (10) when λ = 1:
-
-               Aˆt = δt + (γλ)δt+1 + · · · + · · · + (γλ)T −t+1δT −1,                                 (11)
-
-               where δt = rt + γV (st+1) − V (st)                                                     (12)
-
-A proximal policy optimization (PPO) algorithm that uses ﬁxed-length trajectory segments is
-shown below. Each iteration, each of N (parallel) actors collect T timesteps of data. Then we
-construct the surrogate loss on these N T timesteps of data, and optimize it with minibatch SGD
-(or usually for better performance, Adam [KB14]), for K epochs.
-
-Algorithm 1 PPO, Actor-Critic Style
-
-   for iteration=1, 2, . . . do
-       for actor=1, 2, . . . , N do
-            Run policy πθold in environment for T timesteps
-            Compute advantage estimates Aˆ1, . . . , AˆT
-       end for
-       Optimize surrogate L wrt θ, with K epochs and minibatch size M ≤ N T
-       θold ← θ
-
-   end for
-
-6 Experiments
-
-6.1 Comparison of Surrogate Objectives
-
-First, we compare several diﬀerent surrogate objectives under diﬀerent hyperparameters. Here, we
-compare the surrogate objective LCLIP to several natural variations and ablated versions.
-
-No clipping or penalty:               Lt(θ) = rt(θ)Aˆt
-Clipping:                             Lt(θ) = min(rt(θ)Aˆt, clip(rt(θ)), 1 − , 1 + )Aˆt
-KL penalty (ﬁxed or adaptive)         Lt(θ) = rt(θ)Aˆt − β KL[πθold , πθ]
-
-                                                   5
-For the KL penalty, one can either use a ﬁxed penalty coeﬃcient β or an adaptive coeﬃcient as
-described in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,
-but found the performance to be no better.
-
-    Because we are searching over hyperparameters for each algorithm variant, we chose a compu-
-tationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2
-implemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do
-one million timesteps of training on each one. Besides the hyperparameters used for clipping ( )
-and the KL penalty (β, dtarg), which we search over, the other hyperparameters are provided in in
-Table 3.
-
-    To represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,
-and tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard
-deviations, following [Sch+15b; Dua+16]. We don’t share parameters between the policy and value
-function (so coeﬃcient c1 is irrelevant), and we don’t use an entropy bonus.
-
-    Each algorithm was run on all 7 environments, with 3 random seeds on each. We scored each
-run of the algorithm by computing the average total reward of the last 100 episodes. We shifted
-and scaled the scores for each environment so that the random policy gave a score of 0 and the best
-result was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.
-
-    The results are shown in Table 1. Note that the score is negative for the setting without clipping
-or penalties, because for one environment (half cheetah) it leads to a very negative score, which is
-worse than the initial random policy.
-
-algorithm                  avg. normalized score
-
-No clipping or penalty     -0.39
-Clipping, = 0.1             0.76
-Clipping, = 0.2            0.82
-Clipping, = 0.3             0.70
-Adaptive KL dtarg = 0.003   0.68
-Adaptive KL dtarg = 0.01    0.74
-Adaptive KL dtarg = 0.03    0.71
-Fixed KL, β = 0.3           0.62
-Fixed KL, β = 1.            0.71
-Fixed KL, β = 3.            0.72
-Fixed KL, β = 10.           0.69
-
-Table 1: Results from continuous control benchmark. Average normalized scores (over 21 runs of the
-algorithm, on 7 environments) for each algorithm / hyperparameter setting . β was initialized at 1.
-
-6.2 Comparison to Other Algorithms in the Continuous Domain
-
-Next, we compare PPO (with the “clipped” surrogate objective from Section 3) to several other
-methods from the literature, which are considered to be eﬀective for continuous problems. We com-
-pared against tuned implementations of the following algorithms: trust region policy optimization
-[Sch+15b], cross-entropy method (CEM) [SL06], vanilla policy gradient with adaptive stepsize3,
-
-    2HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, and Walker2d, all “-v1”
-    3After each batch of data, the Adam stepsize is adjusted based on the KL divergence of the original and updated
-policy, using a rule similar to the one shown in Section 4. An implementation is available at https://github.com/
-berkeleydeeprlcourse/homework/tree/master/hw4.
-
-                                                                6
-A2C [Mni+16], A2C with trust region [Wan+16]. A2C stands for advantage actor critic, and is
-a synchronous version of A3C, which we found to have the same or better performance than the
-asynchronous version. For PPO, we used the hyperparameters from the previous section, with
-
-  = 0.2. We see that PPO outperforms the previous methods on almost all the continuous control
-environments.
-
-          HalfCheetah-v1                         Hopper-v1                     InvertedDoublePendulum-v1                   InvertedPendulum-v1
-           Reacher-v1                            Swimmer-v1                                                          1000
-2000                               2500                               8000
-1500
-1000                               2000                                                                   800
- 500
-                                                                      6000
-   0
- 500                               1500                                                                   600
-
-       0                                                              4000                                400
-
-  20                               1000
-  40
-  60                               500                                2000                                200
-  80
- 100                               0                                  0                                   0
- 120                                                                                                         0
-                          1000000        0                   1000000        0                    1000000                                        1000000
-       0                                                                                                       A2C
-                                                                               Walker2d-v1                     A2C + Trust Region
-                                                                                                               CEM
-                                   120                                3000                                     PPO (Clip)
-                                   100                                2000                                     Vanilla PG, Adaptive
-                                    80                                1000                                     TRPO
-                                    60
-                                    40                                0
-                                    20
-
-                                     0
-
-                          1000000        0                   1000000        0                    1000000
-
-Figure 3: Comparison of several algorithms on several MuJoCo environments, training for one million
-timesteps.
-
-6.3 Showcase in the Continuous Domain: Humanoid Running and Steering
-
-To showcase the performance of PPO on high-dimensional continuous control problems, we train
-on a set of problems involving a 3D humanoid, where the robot must run, steer, and get up
-oﬀ the ground, possibly while being pelted by cubes. The three tasks we test on are (1) Ro-
-boschoolHumanoid: forward locomotion only, (2) RoboschoolHumanoidFlagrun: position of target
-is randomly varied every 200 timesteps or whenever the goal is reached, (3) RoboschoolHumanoid-
-FlagrunHarder, where the robot is pelted by cubes and needs to get up oﬀ the ground. See Figure 5
-for still frames of a learned policy, and Figure 4 for learning curves on the three tasks. Hyperpa-
-rameters are provided in Table 4. In concurrent work, Heess et al. [Hee+17] used the adaptive KL
-variant of PPO (Section 4) to learn locomotion policies for 3D robots.
-
-                          RoboschoolHumanoid-v0          RoboschoolHumanoidFlagrun-v0 RoboschoolHumanoidFlagrunHarder-v0
-                                                 2500
-          4000
-          3000                                                                             3000
-          2000                                   2000
-          1000
-                                                 1500                          2000
-             0
-               0                                 1000
-                                                                                           1000
-
-                                                  500
-
-                                   Timestep            0     Timestep                 0                              100M
-                                                 50M 0                         100M 0
-                                                                                                 Timestep
-
-          Figure 4: Learning curves from PPO on 3D humanoid control tasks, using Roboschool.
-
-                                                             7
-Figure 5: Still frames of the policy learned from RoboschoolHumanoidFlagrun. In the ﬁrst six frames, the
-robot runs towards a target. Then the position is randomly changed, and the robot turns and runs toward
-the new target.
-
-6.4 Comparison to Other Algorithms on the Atari Domain
-
-We also ran PPO on the Arcade Learning Environment [Bel+15] benchmark and compared against
-well-tuned implementations of A2C [Mni+16] and ACER [Wan+16]. For all three algorithms, we
-used the same policy network architechture as used in [Mni+16]. The hyperparameters for PPO
-are provided in Table 5. For the other two algorithms, we used hyperparameters that were tuned
-to maximize performance on this benchmark.
-
-    A table of results and learning curves for all 49 games is provided in Appendix B. We consider
-the following two scoring metrics: (1) average reward per episode over entire training period (which
-favors fast learning), and (2) average reward per episode over last 100 episodes of training (which
-favors ﬁnal performance). Table 2 shows the number of games “won” by each algorithm, where we
-compute the victor by averaging the scoring metric across three trials.
-
-                                              A2C ACER PPO Tie
-
-(1) avg. episode reward over all of training  1   18 30 0
-                                                  28 19 1
-(2) avg. episode reward over last 100 episodes 1
-
-Table 2: Number of games “won” by each algorithm, where the scoring metric is averaged across three trials.
-
-7 Conclusion
-
-We have introduced proximal policy optimization, a family of policy optimization methods that use
-multiple epochs of stochastic gradient ascent to perform each policy update. These methods have
-the stability and reliability of trust-region methods but are much simpler to implement, requiring
-only few lines of code change to a vanilla policy gradient implementation, applicable in more general
-settings (for example, when using a joint architecture for the policy and value function), and have
-better overall performance.
-
-8 Acknowledgements
-
-Thanks to Rocky Duan, Peter Chen, and others at OpenAI for insightful comments.
-
-8
-References
-
-[Bel+15]  M. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. “The arcade learning environ-
-          ment: An evaluation platform for general agents”. In: Twenty-Fourth International
-          Joint Conference on Artiﬁcial Intelligence. 2015.
-
-[Bro+16] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W.
-                Zaremba. “OpenAI Gym”. In: arXiv preprint arXiv:1606.01540 (2016).
-
-[Dua+16] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. “Benchmarking Deep
-                Reinforcement Learning for Continuous Control”. In: arXiv preprint arXiv:1604.06778
-                (2016).
-
-[Hee+17]  N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang,
-          A. Eslami, M. Riedmiller, et al. “Emergence of Locomotion Behaviours in Rich Envi-
-          ronments”. In: arXiv preprint arXiv:1707.02286 (2017).
-
-[KL02]    S. Kakade and J. Langford. “Approximately optimal approximate reinforcement learn-
-          ing”. In: ICML. Vol. 2. 2002, pp. 267–274.
-
-[KB14]    D. Kingma and J. Ba. “Adam: A method for stochastic optimization”. In: arXiv
-          preprint arXiv:1412.6980 (2014).
-
-[Mni+15]  V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
-          M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. “Human-level control through deep
-          reinforcement learning”. In: Nature 518.7540 (2015), pp. 529–533.
-
-[Mni+16]  V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and
-          K. Kavukcuoglu. “Asynchronous methods for deep reinforcement learning”. In: arXiv
-          preprint arXiv:1602.01783 (2016).
-
-[Sch+15a] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. “High-dimensional contin-
-                uous control using generalized advantage estimation”. In: arXiv preprint arXiv:1506.02438
-                (2015).
-
-[Sch+15b] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. “Trust region policy
-                optimization”. In: CoRR, abs/1502.05477 (2015).
-
-[SL06]    I. Szita and A. L¨orincz. “Learning Tetris using the noisy cross-entropy method”. In:
-          Neural computation 18.12 (2006), pp. 2936–2941.
-
-[TET12]   E. Todorov, T. Erez, and Y. Tassa. “MuJoCo: A physics engine for model-based con-
-          trol”. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Con-
-          ference on. IEEE. 2012, pp. 5026–5033.
-
-[Wan+16] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas.
-                “Sample Eﬃcient Actor-Critic with Experience Replay”. In: arXiv preprint arXiv:1611.01224
-                (2016).
-
-[Wil92]   R. J. Williams. “Simple statistical gradient-following algorithms for connectionist re-
-          inforcement learning”. In: Machine learning 8.3-4 (1992), pp. 229–256.
-
-            9
-A Hyperparameters
-
-                   Hyperparameter     Value
-                   Horizon (T)
-                   Adam stepsize      2048
-                   Num. epochs        3 × 10−4
-                   Minibatch size     10
-                   Discount (γ)       64
-                   GAE parameter (λ)  0.99
-                                      0.95
-
-Table 3: PPO hyperparameters used for the Mujoco 1 million timestep benchmark.
-
-Hyperparameter                     Value
-Horizon (T)                        512
-Adam stepsize                      ∗
-Num. epochs                        15
-Minibatch size                     4096
-Discount (γ)                       0.99
-GAE parameter (λ)                  0.95
-Number of actors                   32 (locomotion), 128 (ﬂagrun)
-Log stdev. of action distribution  LinearAnneal(−0.7, −1.6)
-
-Table 4: PPO hyperparameters used for the Roboschool experiments. Adam stepsize was adjusted based on
-the target value of the KL divergence.
-
-Hyperparameter                        Value
-Horizon (T)
-Adam stepsize                         128
-Num. epochs                           2.5 × 10−4 × α
-Minibatch size                        3
-Discount (γ)                          32 × 8
-GAE parameter (λ)                     0.99
-Number of actors                      0.95
-Clipping parameter                    8
-VF coeﬀ. c1 (9)                       0.1 × α
-Entropy coeﬀ. c2 (9)                  1
-                                      0.01
-
-Table 5: PPO hyperparameters used in Atari experiments. α is linearly annealed from 1 to 0 over the course
-of learning.
-
-B Performance on More Atari Games
-
-Here we include a comparison of PPO against A2C on a larger collection of 49 Atari games. Figure 6
-shows the learning curves of each of three random seeds, while Table 6 shows the mean performance.
-
-                   10
-         Alien                               Amidar                       Assault                           Asterix                     Asteroids
-                                            BankHeist                    BattleZone         7500                                         Bowling
-2000                       750              Breakout            4000     Centipede          5000                         2500         CrazyClimber
-                                                                2000                        2500                         2000            Freeway
-                           500                                                                                           1500          Jamesbond
-                                                                   0                           0                                       MsPacman
-1000                       250                                 20000                                      BeamRider        50             Qbert
-                                                               15000                                                       40         SpaceInvaders
-                           0                                   10000                        4000                           30           UpNDown
-                                                                5000                        3000
-         Atlantis                                                                           2000                       100000                 A2C
-                                                               10000                        1000                        50000                 ACER
-3000000                    1000                                                                                                               PPO
-                                                                5000                                  ChopperCommand
-2000000                                                                                     6000
-
-1000000                    500                                                              4000
-                                                                                            2000
-0                          0
-
-100      Boxing
-
-                           400
-
-50                         200
-
-    0    DemonAttack           0            DoubleDunk                      Enduro                   FishingDerby
-40000      Frostbite        10.0              Gopher                        Gravitar
-20000                       12.5                Krull            750     KungFuMaster                                           30
-                            15.0               Pitfall           500         Pong           0
-    0                       17.5                                 250       Robotank
-  300                                       RoadRunner                     TimePilot                                            20
-  200                      40000              Tennis               0     WizardOfWor
-                           20000                                           Frames           50                         10
-                                            VideoPinball         750
-                                             Frames              500                        100                        0
-                                                                 250
-                                                               40000                                 IceHockey
-
-                                                               20000                        4                          600
-
-  100                                 0                            0                         6
-            Kangaroo                                              20                                                            400
-10000                                                              0
- 5000                              8000                           20                         8
-                                   6000                                                                                         200
-    0                              4000                            6
-                                   2000                            4                        10
-10000    NameThisGame                                              2
- 7500                                                                                                                  0
- 5000                                 0
- 2500                                                                                                MontezumaRevenge
-10000                               100
- 7500                                                                                       100                        3000
- 5000
- 2500                                                                                       50                          2000
-                                                                                                                        1000
-40000                                                                                        0
-20000                                                                                                  PrivateEye      15000
-
-    0                                                                                       500                        10000
-                                                                                              0                         5000
-   10                                                                                                    Seaquest
-    5                                                                                                                      0
-    0
-          Riverraid        40000                                                            1500                       1000
-      0  StarGunner        20000
-                                                                                            1000
-                               0
-                                                                                            500                        500
-                              10
-                              15                                                              0                        200000
-                              20
-                                                                                                        Tutankham
-                                                                                            300
-
-                                                               4000                         200
-                                                               3000                                                           100000
-
-                                                                                            100
-
-                                                                                            0                                  0
-                                                                                                     Zaxxon
-         Venture           150000                              4000                         6000
-         Frames            100000                                                                                   40M
-                            50000                                                           4000     Frames
-
-                                                               2000                         2000
-
-                                                                                            0
-
-                      40M                0                40M         0                40M        0
-
-Figure 6: Comparison of PPO and A2C on all 49 ATARI games included in OpenAI Gym at the time of
-publication.
-
-                                                                         11
-Alien                  A2C        ACER              PPO
-Amidar
-Assault              1141.7       1655.4         1850.3
-Asterix               380.8        827.6            674.6
-Asteroids            1562.9       4653.8
-Atlantis             3176.3      6801.2          4971.9
-BankHeist            1653.3      2389.3           4532.5
-BattleZone        729265.3    1841376.0           2097.5
-BeamRider            1095.3       1177.5    2311815.0
-Bowling              3080.0       8983.3         1280.6
-Boxing               3031.7      3863.3        17366.7
-Breakout                30.1                      1590.0
-Centipede                            33.3
-ChopperCommand          17.7        98.9            40.1
-CrazyClimber          303.0        456.4             94.6
-DemonAttack          3496.5      8904.8             274.8
-DoubleDunk           1171.7      5287.7           4386.4
-Enduro            107770.0    132461.0            3516.3
-FishingDerby         6639.1    38808.3         110202.0
-Freeway                -16.2       -13.2        11378.4
-Frostbite                                           -14.9
-Gopher                   0.0           0.0        758.3
-Gravitar                20.6        34.7             17.8
-IceHockey                0.0                        32.5
-Jamesbond             261.8            0.0        314.2
-Kangaroo             1500.9         285.6         2932.9
-Krull                 194.0    37802.3            737.2
-KungFuMaster            -6.4        225.3            -4.2
-MontezumaRevenge        52.3                      560.7
-MsPacman                45.3          -5.9       9928.7
-NameThisGame       8367.4           261.8         7942.3
-Pitfall            24900.3                      23310.3
-Pong                                 50.0           42.0
-PrivateEye               0.0      7268.4          2096.5
-Qbert                1626.9    27599.3            6254.9
-Riverraid            5961.2                         -32.9
-RoadRunner                             0.3           20.7
-Robotank               -55.0     2718.5              69.5
-Seaquest                19.7     8488.0         14293.3
-SpaceInvaders           91.3                      8393.6
-StarGunner         10065.7         -16.9        25076.0
-Tennis               7653.5         20.7              5.5
-TimePilot          32810.0         182.0          1204.5
-Tutankham                2.2   15316.6              942.5
-UpNDown              1714.3      9125.1         32689.0
-Venture               744.5    35466.0             -14.8
-VideoPinball       26204.0                       4342.0
-WizardOfWor            -22.2           2.5          254.4
-Zaxxon               2898.0      1739.5         95445.0
-                      206.8      1213.9
-                   17369.8     49817.7                 0.0
-                                                37389.0
-                         0.0        -17.6        4185.3
-                   19735.9        4175.7         5008.7
-                                   280.8
-                      859.0   145051.4
-                        16.3
-                                       0.0
-                              156225.6
-
-                                  2308.3
-                                     29.0
-
-Table 6: Mean ﬁnal scores (last 100 episodes) of PPO and A2C on Atari games after 40M game frames (10M
-timesteps).
-
-                  12
-
diff --git a/reference papers/SAC_paper.pdf b/reference papers/SAC_paper.pdf
deleted file mode 100644
index 9f87f20..0000000
--- a/reference papers/SAC_paper.pdf	
+++ /dev/null
@@ -1,806 +0,0 @@
-                                                       Soft Actor-Critic:
-
-                                       Off-Policy Maximum Entropy Deep Reinforcement
-
-                                                  Learning with a Stochastic Actor
-
-                                       Tuomas Haarnoja 1 Aurick Zhou 1 Pieter Abbeel 1 Sergey Levine 1
-
-arXiv:1801.01290v2 [cs.LG] 8 Aug 2018                          Abstract                                  of these methods in real-world domains has been hampered
-                                                                                                         by two major challenges. First, model-free deep RL meth-
-                                             Model-free deep reinforcement learning (RL) al-             ods are notoriously expensive in terms of their sample com-
-                                             gorithms have been demonstrated on a range of               plexity. Even relatively simple tasks can require millions of
-                                             challenging decision making and control tasks.              steps of data collection, and complex behaviors with high-
-                                             However, these methods typically suffer from two            dimensional observations might need substantially more.
-                                             major challenges: very high sample complexity               Second, these methods are often brittle with respect to their
-                                             and brittle convergence properties, which necessi-          hyperparameters: learning rates, exploration constants, and
-                                             tate meticulous hyperparameter tuning. Both of              other settings must be set carefully for different problem
-                                             these challenges severely limit the applicability           settings to achieve good results. Both of these challenges
-                                             of such methods to complex, real-world domains.             severely limit the applicability of model-free deep RL to
-                                             In this paper, we propose soft actor-critic, an off-        real-world tasks.
-                                             policy actor-critic deep RL algorithm based on the
-                                             maximum entropy reinforcement learning frame-               One cause for the poor sample efﬁciency of deep RL meth-
-                                            work. In this framework, the actor aims to maxi-             ods is on-policy learning: some of the most commonly used
-                                             mize expected reward while also maximizing en-              deep RL algorithms, such as TRPO (Schulman et al., 2015),
-                                             tropy. That is, to succeed at the task while acting         PPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),
-                                             as randomly as possible. Prior deep RL methods              require new samples to be collected for each gradient step.
-                                             based on this framework have been formulated                This quickly becomes extravagantly expensive, as the num-
-                                             as Q-learning methods. By combining off-policy              ber of gradient steps and samples per step needed to learn
-                                             updates with a stable stochastic actor-critic formu-        an effective policy increases with task complexity. Off-
-                                             lation, our method achieves state-of-the-art per-           policy algorithms aim to reuse past experience. This is not
-                                             formance on a range of continuous control bench-            directly feasible with conventional policy gradient formula-
-                                             mark tasks, outperforming prior on-policy and               tions, but is relatively straightforward for Q-learning based
-                                             off-policy methods. Furthermore, we demonstrate             methods (Mnih et al., 2015). Unfortunately, the combina-
-                                             that, in contrast to other off-policy algorithms, our       tion of off-policy learning and high-dimensional, nonlinear
-                                             approach is very stable, achieving very similar             function approximation with neural networks presents a ma-
-                                             performance across different random seeds.                  jor challenge for stability and convergence (Bhatnagar et al.,
-                                                                                                         2009). This challenge is further exacerbated in continuous
-                                       1. Introduction                                                   state and action spaces, where a separate actor network is
-                                                                                                         often used to perform the maximization in Q-learning. A
-                                       Model-free deep reinforcement learning (RL) algorithms            commonly used algorithm in such settings, deep determinis-
-                                       have been applied in a range of challenging domains, from         tic policy gradient (DDPG) (Lillicrap et al., 2015), provides
-                                       games (Mnih et al., 2013; Silver et al., 2016) to robotic         for sample-efﬁcient learning but is notoriously challenging
-                                       control (Schulman et al., 2015). The combination of RL            to use due to its extreme brittleness and hyperparameter
-                                       and high-capacity function approximators such as neural           sensitivity (Duan et al., 2016; Henderson et al., 2017).
-                                       networks holds the promise of automating a wide range of
-                                       decision making and control tasks, but widespread adoption        We explore how to design an efﬁcient and stable model-
-                                                                                                         free deep RL algorithm for continuous state and action
-                                          1Berkeley Artiﬁcial Intelligence Research, University of Cal-  spaces. To that end, we draw on the maximum entropy
-                                       ifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja        framework, which augments the standard maximum reward
-                                       <haarnoja@berkeley.edu>.                                          reinforcement learning objective with an entropy maximiza-
-                                                                                                         tion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,
-Soft Actor-Critic
-
-2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-    and policy improvement—using the value function to obtain
-tropy reinforcement learning alters the RL objective, though   a better policy (Barto et al., 1983; Sutton & Barto, 1998). In
-the original objective can be recovered using a tempera-       large-scale reinforcement learning problems, it is typically
-ture parameter (Haarnoja et al., 2017). More importantly,      impractical to run either of these steps to convergence, and
-the maximum entropy formulation provides a substantial         instead the value function and policy are optimized jointly.
-improvement in exploration and robustness: as discussed        In this case, the policy is referred to as the actor, and the
-by Ziebart (2010), maximum entropy policies are robust         value function as the critic. Many actor-critic algorithms
-in the face of model and estimation errors, and as demon-      build on the standard, on-policy policy gradient formulation
-strated by (Haarnoja et al., 2017), they improve exploration   to update the actor (Peters & Schaal, 2008), and many of
-by acquiring diverse behaviors. Prior work has proposed        them also consider the entropy of the policy, but instead of
-model-free deep RL algorithms that perform on-policy learn-    maximizing the entropy, they use it as an regularizer (Schul-
-ing with entropy maximization (O’Donoghue et al., 2016),       man et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,
-as well as off-policy methods based on soft Q-learning and     2017). On-policy training tends to improve stability but
-its variants (Schulman et al., 2017a; Nachum et al., 2017a;    results in poor sample complexity.
-Haarnoja et al., 2017). However, the on-policy variants suf-
-fer from poor sample complexity for the reasons discussed      There have been efforts to increase the sample efﬁciency
-above, while the off-policy variants require complex approx-   while retaining robustness by incorporating off-policy sam-
-imate inference procedures in continuous action spaces.        ples and by using higher order variance reduction tech-
-                                                               niques (O’Donoghue et al., 2016; Gu et al., 2016). How-
-In this paper, we demonstrate that we can devise an off-       ever, fully off-policy algorithms still attain better efﬁ-
-policy maximum entropy actor-critic algorithm, which we        ciency. A particularly popular off-policy actor-critic method,
-call soft actor-critic (SAC), which provides for both sample-  DDPG (Lillicrap et al., 2015), which is a deep variant of the
-efﬁcient learning and stability. This algorithm extends read-  deterministic policy gradient (Silver et al., 2014) algorithm,
-ily to very complex, high-dimensional tasks, such as the       uses a Q-function estimator to enable off-policy learning,
-Humanoid benchmark (Duan et al., 2016) with 21 action          and a deterministic actor that maximizes this Q-function.
-dimensions, where off-policy methods such as DDPG typi-        As such, this method can be viewed both as a determinis-
-cally struggle to obtain good results (Gu et al., 2016). SAC   tic actor-critic algorithm and an approximate Q-learning
-also avoids the complexity and potential instability associ-   algorithm. Unfortunately, the interplay between the deter-
-ated with approximate inference in prior off-policy maxi-      ministic actor network and the Q-function typically makes
-mum entropy algorithms based on soft Q-learning (Haarnoja      DDPG extremely difﬁcult to stabilize and brittle to hyperpa-
-et al., 2017). We present a convergence proof for policy       rameter settings (Duan et al., 2016; Henderson et al., 2017).
-iteration in the maximum entropy framework, and then in-       As a consequence, it is difﬁcult to extend DDPG to complex,
-troduce a new algorithm based on an approximation to this      high-dimensional tasks, and on-policy policy gradient meth-
-procedure that can be practically implemented with deep        ods still tend to produce the best results in such settings (Gu
-neural networks, which we call soft actor-critic. We present   et al., 2016). Our method instead combines off-policy actor-
-empirical results that show that soft actor-critic attains a   critic training with a stochastic actor, and further aims to
-substantial improvement in both performance and sample         maximize the entropy of this actor with an entropy maxi-
-efﬁciency over both off-policy and on-policy prior methods.    mization objective. We ﬁnd that this actually results in a
-We also compare to twin delayed deep deterministic (TD3)       considerably more stable and scalable algorithm that, in
-policy gradient algorithm (Fujimoto et al., 2018), which is    practice, exceeds both the efﬁciency and ﬁnal performance
-a concurrent work that proposes a deterministic algorithm      of DDPG. A similar method can be derived as a zero-step
-that substantially improves on DDPG.                           special case of stochastic value gradients (SVG(0)) (Heess
-                                                               et al., 2015). However, SVG(0) differs from our method in
-2. Related Work                                                that it optimizes the standard maximum expected return ob-
-                                                               jective, and it does not make use of a separate value network,
-Our soft actor-critic algorithm incorporates three key in-     which we found to make training more stable.
-gredients: an actor-critic architecture with separate policy
-and value function networks, an off-policy formulation that    Maximum entropy reinforcement learning optimizes poli-
-enables reuse of previously collected data for efﬁciency, and  cies to maximize both the expected return and the ex-
-entropy maximization to enable stability and exploration.      pected entropy of the policy. This framework has been
-We review prior works that draw on some of these ideas in      used in many contexts, from inverse reinforcement learn-
-this section. Actor-critic algorithms are typically derived    ing (Ziebart et al., 2008) to optimal control (Todorov, 2008;
-starting from policy iteration, which alternates between pol-  Toussaint, 2009; Rawlik et al., 2012). In guided policy
-icy evaluation—computing the value function for a policy—      search (Levine & Koltun, 2013; Levine et al., 2016), the
-                                                               maximum entropy distribution is used to guide policy learn-
-Soft Actor-Critic
-
-ing towards high-reward regions. More recently, several        with the expected entropy of the policy over ρπ(st):
-papers have noted the connection between Q-learning and
-policy gradient methods in the framework of maximum en-                            T
-tropy learning (O’Donoghue et al., 2016; Haarnoja et al.,
-2017; Nachum et al., 2017a; Schulman et al., 2017a). While       J (π) = E(st,at)∼ρπ [r(st, at) + αH(π( · |st))] . (1)
-most of the prior model-free works assume a discrete action
-space, Nachum et al. (2017b) approximate the maximum en-                         t=0
-tropy distribution with a Gaussian and Haarnoja et al. (2017)
-with a sampling network trained to draw samples from the       The temperature parameter α determines the relative im-
-optimal policy. Although the soft Q-learning algorithm pro-    portance of the entropy term against the reward, and thus
-posed by Haarnoja et al. (2017) has a value function and       controls the stochasticity of the optimal policy. The maxi-
-actor network, it is not a true actor-critic algorithm: the    mum entropy objective differs from the standard maximum
-Q-function is estimating the optimal Q-function, and the       expected reward objective used in conventional reinforce-
-actor does not directly affect the Q-function except through   ment learning, though the conventional objective can be
-the data distribution. Hence, Haarnoja et al. (2017) moti-     recovered in the limit as α → 0. For the rest of this paper,
-vates the actor network as an approximate sampler, rather      we will omit writing the temperature explicitly, as it can
-than the actor in an actor-critic algorithm. Crucially, the    always be subsumed into the reward by scaling it by α−1.
-convergence of this method hinges on how well this sampler
-approximates the true posterior. In contrast, we prove that    This objective has a number of conceptual and practical
-our method converges to the optimal policy from a given        advantages. First, the policy is incentivized to explore more
-policy class, regardless of the policy parameterization. Fur-  widely, while giving up on clearly unpromising avenues.
-thermore, these prior maximum entropy methods generally        Second, the policy can capture multiple modes of near-
-do not exceed the performance of state-of-the-art off-policy   optimal behavior. In problem settings where multiple ac-
-algorithms, such as DDPG, when learning from scratch,          tions seem equally attractive, the policy will commit equal
-though they may have other beneﬁts, such as improved ex-       probability mass to those actions. Lastly, prior work has ob-
-ploration and ease of ﬁne-tuning. In our experiments, we       served improved exploration with this objective (Haarnoja
-demonstrate that our soft actor-critic algorithm does in fact  et al., 2017; Schulman et al., 2017a), and in our experi-
-exceed the performance of prior state-of-the-art off-policy    ments, we observe that it considerably improves learning
-deep RL methods by a wide margin.                              speed over state-of-art methods that optimize the conven-
-                                                               tional RL objective function. We can extend the objective to
-3. Preliminaries                                               inﬁnite horizon problems by introducing a discount factor γ
-                                                               to ensure that the sum of expected rewards and entropies is
-We ﬁrst introduce notation and summarize the standard and      ﬁnite. Writing down the maximum entropy objective for the
-maximum entropy reinforcement learning frameworks.             inﬁnite horizon discounted case is more involved (Thomas,
-                                                               2014) and is deferred to Appendix A.
-3.1. Notation
-                                                               Prior methods have proposed directly solving for the op-
-We address policy learning in continuous action spaces.        timal Q-function, from which the optimal policy can be
-We consider an inﬁnite-horizon Markov decision process         recovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja
-(MDP), deﬁned by the tuple (S, A, p, r), where the state       et al., 2017). We will discuss how we can devise a soft
-space S and the action space A are continuous, and the         actor-critic algorithm through a policy iteration formulation,
-unknown state transition probability p : S × S × A →           where we instead evaluate the Q-function of the current
-[0, ∞) represents the probability density of the next state    policy and update the policy through an off-policy gradient
-st+1 ∈ S given the current state st ∈ S and action at ∈ A.     update. Though such algorithms have previously been pro-
-The environment emits a bounded reward r : S × A →             posed for conventional reinforcement learning, our method
-[rmin, rmax] on each transition. We will use ρπ(st) and        is, to our knowledge, the ﬁrst off-policy actor-critic method
-ρπ(st, at) to denote the state and state-action marginals of   in the maximum entropy reinforcement learning framework.
-the trajectory distribution induced by a policy π(at|st).
-                                                               4. From Soft Policy Iteration to Soft
-3.2. Maximum Entropy Reinforcement Learning                       Actor-Critic
-
-Standard RL maximizes the expected sum of rewards              Our off-policy soft actor-critic algorithm can be derived
-   t E(st,at)∼ρπ [r(st, at)]. We will consider a more gen-     starting from a maximum entropy variant of the policy it-
-                                                               eration method. We will ﬁrst present this derivation, verify
-eral maximum entropy objective (see e.g. Ziebart (2010)),      that the corresponding algorithm converges to the optimal
-which favors stochastic policies by augmenting the objective   policy from its density class, and then present a practical
-                                                               deep reinforcement learning algorithm based on this theory.
-                               Soft Actor-Critic
-
-4.1. Derivation of Soft Policy Iteration                        The partition function Zπold (st) normalizes the distribution,
-                                                                and while it is intractable in general, it does not contribute to
-We will begin by deriving soft policy iteration, a general al-  the gradient with respect to the new policy and can thus be
-gorithm for learning optimal maximum entropy policies that      ignored, as noted in the next section. For this projection, we
-alternates between policy evaluation and policy improve-        can show that the new, projected policy has a higher value
-ment in the maximum entropy framework. Our derivation           than the old policy with respect to the objective in Equa-
-is based on a tabular setting, to enable theoretical analysis   tion 1. We formalize this result in Lemma 2.
-and convergence guarantees, and we extend this method
-into the general continuous setting in the next section. We     Lemma 2 (Soft Policy Improvement). Let πold ∈ Π and let
-will show that soft policy iteration converges to the optimal   πnew be the optimizer of the minimization problem deﬁned
-policy within a set of policies which might correspond, for     in Equation 4. Then Qπnew (st, at) ≥ Qπold (st, at) for all
-instance, to a set of parameterized densities.                  (st, at) ∈ S × A with |A| < ∞.
-
-In the policy evaluation step of soft policy iteration, we      Proof. See Appendix B.2.
-wish to compute the value of a policy π according to the
-maximum entropy objective in Equation 1. For a ﬁxed             The full soft policy iteration algorithm alternates between
-policy, the soft Q-value can be computed iteratively, starting  the soft policy evaluation and the soft policy improvement
-from any function Q : S × A → R and repeatedly applying         steps, and it will provably converge to the optimal maxi-
-a modiﬁed Bellman backup operator T π given by                  mum entropy policy among the policies in Π (Theorem 1).
-                                                                Although this algorithm will provably ﬁnd the optimal solu-
-    T πQ(st, at) r(st, at) + γ Est+1∼p [V (st+1)] , (2)         tion, we can perform it in its exact form only in the tabular
-                                                                case. Therefore, we will next approximate the algorithm for
-where                                                           continuous domains, where we need to rely on a function
-                                                                approximator to represent the Q-values, and running the
-          V (st) = Eat∼π [Q(st, at) − log π(at|st)] (3)         two steps until convergence would be computationally too
-                                                                expensive. The approximation gives rise to a new practical
-is the soft state value function. We can obtain the soft value  algorithm, called soft actor-critic.
-function for any policy π by repeatedly applying T π as
-formalized below.                                               Theorem 1 (Soft Policy Iteration). Repeated application of
-                                                                soft policy evaluation and soft policy improvement from any
-Lemma 1 (Soft Policy Evaluation). Consider the soft Bell-       π ∈ Π converges to a policy π∗ such that Qπ∗ (st, at) ≥
-man backup operator T π in Equation 2 and a mapping             Qπ(st, at) for all π ∈ Π and (st, at) ∈ S × A, assuming
-Q0 : S ×A → R with |A| < ∞, and deﬁne Qk+1 = T πQk.             |A| < ∞.
-Then the sequence Qk will converge to the soft Q-value of
-π as k → ∞.                                                     Proof. See Appendix B.3.
-
-Proof. See Appendix B.1.                                        4.2. Soft Actor-Critic
-
-In the policy improvement step, we update the policy to-        As discussed above, large continuous domains require us to
-wards the exponential of the new Q-function. This particular    derive a practical approximation to soft policy iteration. To
-choice of update can be guaranteed to result in an improved     that end, we will use function approximators for both the
-policy in terms of its soft value. Since in practice we prefer  Q-function and the policy, and instead of running evaluation
-policies that are tractable, we will additionally restrict the  and improvement to convergence, alternate between opti-
-policy to some set of policies Π, which can correspond, for     mizing both networks with stochastic gradient descent. We
-example, to a parameterized family of distributions such as     will consider a parameterized state value function Vψ(st),
-Gaussians. To account for the constraint that π ∈ Π, we         soft Q-function Qθ(st, at), and a tractable policy πφ(at|st).
-project the improved policy into the desired set of policies.   The parameters of these networks are ψ, θ, and φ. For
-While in principle we could choose any projection, it will      example, the value functions can be modeled as expressive
-turn out to be convenient to use the information projection     neural networks, and the policy as a Gaussian with mean
-deﬁned in terms of the Kullback-Leibler divergence. In the      and covariance given by neural networks. We will next
-other words, in the policy improvement step, for each state,    derive update rules for these parameter vectors.
-we update the policy according to
-                                                                The state value function approximates the soft value. There
-πnew = arg min DKL π ( · |st)  exp (Qπold (st, · ))  .          is no need in principle to include a separate function approx-
-                                    Zπold (st)                  imator for the state value, since it is related to the Q-function
-                     π ∈Π                                       and policy according to Equation 3. This quantity can be
-
-                                                     (4)
-                                     Soft Actor-Critic
-
-estimated from a single action sample from the current pol-             Algorithm 1 Soft Actor-Critic
-icy without introducing a bias, but in practice, including a               Initialize parameter vectors ψ, ψ¯, θ, φ.
-separate function approximator for the soft value can stabi-               for each iteration do
-lize training and is convenient to train simultaneously with                  for each environment step do
-the other networks. The soft value function is trained to                        at ∼ πφ(at|st)
-minimize the squared residual error                                              st+1 ∼ p(st+1|st, at)
-                                                                                 D ← D ∪ {(st, at, r(st, at), st+1)}
-JV (ψ) = Est∼D  1  Vψ(st) − Eat∼πφ [Qθ(st, at) − log πφ(at|st)] 2             end for
-                2                                                             for each gradient step do
-                                                                                 ψ ← ψ − λV ∇ˆ ψJV (ψ)
-                                                       (5)                       θi ← θi − λQ∇ˆ θi JQ(θi) for i ∈ {1, 2}
-                                                                                 φ ← φ − λπ∇ˆ φJπ(φ)
-where D is the distribution of previously sampled states and                     ψ¯ ← τ ψ + (1 − τ )ψ¯
-actions, or a replay buffer. The gradient of Equation 5 can                   end for
-be estimated with an unbiased estimator                                    end for
-
-∇ˆ ψJV (ψ) = ∇ψVψ(st) (Vψ(st) − Qθ(st, at) + log πφ(at|st)) ,           where t is an input noise vector, sampled from some ﬁxed
-                                                                   (6)  distribution, such as a spherical Gaussian. We can now
-                                                                        rewrite the objective in Equation 10 as
-where the actions are sampled according to the current pol-
-icy, instead of the replay buffer. The soft Q-function param-           Jπ(φ) = Est∼D, t∼N [log πφ(fφ( t; st)|st) − Qθ(st, fφ( t; st))] ,
-eters can be trained to minimize the soft Bellman residual
-                                                                                                                                          (12)
-JQ(θ) = E(st,at)∼D  1  Qθ(st, at) − Qˆ(st, at) 2       ,
-                    2                                                   where πφ is deﬁned implicitly in terms of fφ, and we have
-                                                                        noted that the partition function is independent of φ and can
-                                                       (7)              thus be omitted. We can approximate the gradient of Equa-
-                                                                        tion 12 with
-with
-     Qˆ(st, at) = r(st, at) + γ Est+1∼p Vψ¯(st+1) , (8)                 ∇ˆ φJπ(φ) = ∇φ log πφ(at|st)
-                                                                              + (∇at log πφ(at|st) − ∇at Q(st, at))∇φfφ( t; st),
-which again can be optimized with stochastic gradients                                                                                    (13)
-
-∇ˆ θJQ(θ) = ∇θQθ(at, st) Qθ(st, at) − r(st, at) − γVψ¯(st+1) .          where at is evaluated at fφ( t; st). This unbiased gradient
-                                                                   (9)  estimator extends the DDPG style policy gradients (Lillicrap
-                                                                        et al., 2015) to any tractable stochastic policy.
-The update makes use of a target value network Vψ¯, where
-ψ¯ can be an exponentially moving average of the value                  Our algorithm also makes use of two Q-functions to mitigate
-network weights, which has been shown to stabilize train-               positive bias in the policy improvement step that is known
-ing (Mnih et al., 2015). Alternatively, we can update the               to degrade performance of value based methods (Hasselt,
-target weights to match the current value function weights              2010; Fujimoto et al., 2018). In particular, we parameterize
-periodically (see Appendix E). Finally, the policy param-               two Q-functions, with parameters θi, and train them inde-
-eters can be learned by directly minimizing the expected                pendently to optimize JQ(θi). We then use the minimum of
-KL-divergence in Equation 4:                                            the Q-functions for the value gradient in Equation 6 and pol-
-                                                                        icy gradient in Equation 13, as proposed by Fujimoto et al.
-Jπ(φ) = Est∼D DKL πφ( · |st)         exp (Qθ(st, · ))              .    (2018). Although our algorithm can learn challenging tasks,
-                                          Zθ (st )                      including a 21-dimensional Humanoid, using just a single
-                                                                        Q-function, we found two Q-functions signiﬁcantly speed
-                                                       (10)             up training, especially on harder tasks. The complete algo-
-                                                                        rithm is described in Algorithm 1. The method alternates
-There are several options for minimizing Jπ. A typical                  between collecting experience from the environment with
-solution for policy gradient methods is to use the likelihood           the current policy and updating the function approximators
-ratio gradient estimator (Williams, 1992), which does not               using the stochastic gradients from batches sampled from a
-require backpropagating the gradient through the policy and             replay buffer. In practice, we take a single environment step
-the target density networks. However, in our case, the target           followed by one or several gradient steps (see Appendix D
-density is the Q-function, which is represented by a neural
-network an can be differentiated, and it is thus convenient
-to apply the reparameterization trick instead, resulting in a
-lower variance estimator. To that end, we reparameterize
-the policy using a neural network transformation
-
-                   at = fφ( t; st),                    (11)
-                                                                                          Soft Actor-Critic
-
-                                          Hopper-v1                                                     Walker2d-v1                                         HalfCheetah-v1
-
-                4000                                                           6000
-                                                                               5000
-                                                                               4000                                                            15000
-
-                3000
-
-average return                                                 average return                                                  average return  10000
-
-                2000                                                           3000
-                                                                               2000
-                1000                                                           1000                                                            5000
-
-                0                                                              0                                                               0
-                                                                                                                                                0.0 0.5 1.0 1.5 2.0 2.5 3.0
-                      0.0  0.2  0.4  0.6             0.8  1.0                        0.0  0.2  0.4  0.6              0.8  1.0                                         million steps
-                                million steps                                                  million steps
-
-                           (a) Hopper-v1                                                  (b) Walker2d-v1                                                (c) HalfCheetah-v1
-
-                                     Ant-v1                                                       Humanoid-v1                                                    Humanoid (rllab)
-
-                6000                                                           8000                                                            6000      SAC
-                                                                                                                                               4000      DDPG
-average return  4000                                           average return  6000                                            average return            PPO
-                                                                                                                                                         SQL
-                                                                                                                                                         TD3 (concurrent)
-
-                                                                               4000
-
-                2000                                                                                                                           2000
-
-                                                                               2000
-
-                0                                                              0                                                               0
-                 0.0 0.5 1.0 1.5 2.0 2.5 3.0
-                                       million steps                                 0    2    4    6                8    10                          0  2  4              6       8  10
-                                                                                               million steps                                                million steps
-                            (d) Ant-v1
-                                                                                          (e) Humanoid-v1                                                (f) Humanoid (rllab)
-
-Figure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and
-outperforming both on-policy and off-policy methods in the most challenging tasks.
-
-for all hyperparameter). Using off-policy data from a replay                                        We compare our method to deep deterministic policy gra-
-buffer is feasible because both value estimators and the pol-                                       dient (DDPG) (Lillicrap et al., 2015), an algorithm that
-icy can be trained entirely on off-policy data. The algorithm                                       is regarded as one of the more efﬁcient off-policy deep
-is agnostic to the parameterization of the policy, as long as                                       RL methods (Duan et al., 2016); proximal policy optimiza-
-it can be evaluated for any arbitrary state-action tuple.                                           tion (PPO) (Schulman et al., 2017b), a stable and effective
-                                                                                                    on-policy policy gradient algorithm; and soft Q-learning
-5. Experiments                                                                                      (SQL) (Haarnoja et al., 2017), a recent off-policy algorithm
-                                                                                                    for learning maximum entropy policies. Our SQL imple-
-The goal of our experimental evaluation is to understand                                            mentation also includes two Q-functions, which we found
-how the sample complexity and stability of our method                                               to improve its performance in most environments. We addi-
-compares with prior off-policy and on-policy deep rein-                                             tionally compare to twin delayed deep deterministic policy
-forcement learning algorithms. We compare our method                                                gradient algorithm (TD3) (Fujimoto et al., 2018), using
-to prior techniques on a range of challenging continuous                                            the author-provided implementation. This is an extension
-control tasks from the OpenAI gym benchmark suite (Brock-                                           to DDPG, proposed concurrently to our method, that ﬁrst
-man et al., 2016) and also on the rllab implementation of                                           applied the double Q-learning trick to continuous control
-the Humanoid task (Duan et al., 2016). Although the easier                                          along with other improvements. We have included trust re-
-tasks can be solved by a wide range of different algorithms,                                        gion path consistency learning (Trust-PCL) (Nachum et al.,
-the more complex benchmarks, such as the 21-dimensional                                             2017b) and two other variants of SAC in Appendix E. We
-Humanoid (rllab), are exceptionally difﬁcult to solve with                                          turned off the exploration noise for evaluation for DDPG
-off-policy algorithms (Duan et al., 2016). The stability of                                         and PPO. For maximum entropy algorithms, which do not
-the algorithm also plays a large role in performance: eas-                                          explicitly inject exploration noise, we either evaluated with
-ier tasks make it more practical to tune hyperparameters                                            the exploration noise (SQL) or use the mean action (SAC).
-to achieve good results, while the already narrow basins of                                         The source code of our SAC implementation1 and videos2
-effective hyperparameters become prohibitively small for                                            are available online.
-the more sensitive algorithms on the hardest benchmarks,
-leading to poor performance (Gu et al., 2016).                                                          1github.com/haarnoja/sac
-                                                                                                        2sites.google.com/view/soft-actor-critic
-Soft Actor-Critic
-
-5.1. Comparative Evaluation                                                                 Humanoid (rllab)
-
-Figure 1 shows the total average return of evaluation rollouts                  6000     stochastic policy
-during training for DDPG, PPO, and TD3. We train ﬁve                                     deterministic policy
-different instances of each algorithm with different random
-seeds, with each performing one evaluation rollout every        average return  4000
-1000 environment steps. The solid curves corresponds to the
-mean and the shaded region to the minimum and maximum                           2000
-returns over the ﬁve trials.
-                                                                                0
-The results show that, overall, SAC performs comparably
-to the baseline methods on the easier tasks and outperforms                           0  2  4                  6  8  10
-them on the harder tasks with a large margin, both in terms                                 million steps
-of learning speed and the ﬁnal performance. For example,
-DDPG fails to make any progress on Ant-v1, Humanoid-            Figure 2. Comparison of SAC (blue) and a deterministic variant of
-v1, and Humanoid (rllab), a result that is corroborated by      SAC (red) in terms of the stability of individual random seeds on
-prior work (Gu et al., 2016; Duan et al., 2016). SAC also       the Humanoid (rllab) benchmark. The comparison indicates that
-learns considerably faster than PPO as a consequence of         stochasticity can stabilize training as the variability between the
-the large batch sizes PPO needs to learn stably on more         seeds becomes much higher with a deterministic policy.
-high-dimensional and complex tasks. Another maximum
-entropy RL algorithm, SQL, can also learn all tasks, but it     seeds. Soft actor-critic performs much more consistently,
-is slower than SAC and has worse asymptotic performance.        while the deterministic variant exhibits very high variability
-The quantitative results attained by SAC in our experiments     across seeds, indicating substantially worse stability. As
-also compare very favorably to results reported by other        evident from the ﬁgure, learning a stochastic policy with
-methods in prior work (Duan et al., 2016; Gu et al., 2016;      entropy maximization can drastically stabilize training. This
-Henderson et al., 2017), indicating that both the sample        becomes especially important with harder tasks, where tun-
-efﬁciency and ﬁnal performance of SAC on these benchmark        ing hyperparameters is challenging. In this comparison, we
-tasks exceeds the state of the art. All hyperparameters used    updated the target value network weights with hard updates,
-in this experiment for SAC are listed in Appendix D.            by periodically overwriting the target network parameters
-                                                                to match the current value network (see Appendix E for
-5.2. Ablation Study                                             a comparison of average performance on all benchmark
-                                                                tasks).
-The results in the previous section suggest that algorithms
-based on the maximum entropy principle can outperform           Policy evaluation. Since SAC converges to stochastic
-conventional RL methods on challenging tasks such as the        policies, it is often beneﬁcial to make the ﬁnal policy deter-
-humanoid tasks. In this section, we further examine which       ministic at the end for best performance. For evaluation, we
-particular components of SAC are important for good perfor-     approximate the maximum a posteriori action by choosing
-mance. We also examine how sensitive SAC is to some of          the mean of the policy distribution. Figure 3(a) compares
-the most important hyperparameters, namely reward scaling       training returns to evaluation returns obtained with this strat-
-and target value update smoothing constant.                     egy indicating that deterministic evaluation can yield better
-                                                                performance. It should be noted that all of the training
-Stochastic vs. deterministic policy. Soft actor-critic          curves depict the sum of rewards, which is different from
-learns stochastic policies via a maximum entropy objec-         the objective optimized by SAC and other maximum en-
-tive. The entropy appears in both the policy and value          tropy RL algorithms, including SQL and Trust-PCL, which
-function. In the policy, it prevents premature convergence of   maximize also the entropy of the policy.
-the policy variance (Equation 10). In the value function, it
-encourages exploration by increasing the value of regions of    Reward scale. Soft actor-critic is particularly sensitive to
-state space that lead to high-entropy behavior (Equation 5).    the scaling of the reward signal, because it serves the role
-To compare how the stochasticity of the policy and entropy      of the temperature of the energy-based optimal policy and
-maximization affects the performance, we compare to a           thus controls its stochasticity. Larger reward magnitudes
-deterministic variant of SAC that does not maximize the en-     correspond to lower entries. Figure 3(b) shows how learn-
-tropy and that closely resembles DDPG, with the exception       ing performance changes when the reward scale is varied:
-of having two Q-functions, using hard target updates, not       For small reward magnitudes, the policy becomes nearly
-having a separate target actor, and using ﬁxed rather than      uniform, and consequently fails to exploit the reward signal,
-learned exploration noise. Figure 2 compares ﬁve individual     resulting in substantial degradation of performance. For
-runs with both variants, initialized with different random      large reward magnitudes, the model learns quickly at ﬁrst,
-                                                 Soft Actor-Critic
-
-                            Ant-v1                               Ant-v1                                                                                                        Ant-v1
-
-6000                                       6000  1                                                                                                                      6000                           0.0001
-                                                 3                                                                                                                      4000                           0.001
-                                                                                                                                                                        2000                           0.01
-                                                 10                                                                                                                                                    0.1
-average return                                                                                                                                                              0
-                                                                            average return4000400030                                                                  −2000
-                                                                                                                                                        average return
-                                                 100
-
-2000                                       2000
-
-0     deterministic evaluation             0
-                                            0.0 0.5 1.0 1.5 2.0 2.5 3.0
-      stochastic evaluation                                       million steps
-
-      0.0 0.5 1.0 1.5 2.0 2.5 3.0                  (b) Reward Scale                                                                                                           0.0 0.5 1.0 1.5 2.0 2.5 3.0
-                            million steps                                                                                                                                                           million steps
-
-      (a) Evaluation                                                                                                                                                    (c) Target Smoothing Coefﬁcient (τ )
-
-Figure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action
-generally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,
-correspond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the
-temperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.
-(c) Target value smoothing coefﬁcient τ is used to stabilize training. Fast moving target (large τ ) can result in instabilities (red), whereas
-slow moving target (small τ ) makes training slower (blue).
-
-but the policy then becomes nearly deterministic, leading        6. Conclusion
-to poor local minima due to lack of adequate exploration.
-With the right reward scaling, the model balances explo-         We present soft actor-critic (SAC), an off-policy maximum
-ration and exploitation, leading to faster learning and better   entropy deep reinforcement learning algorithm that provides
-asymptotic performance. In practice, we found reward scale       sample-efﬁcient learning while retaining the beneﬁts of en-
-to be the only hyperparameter that requires tuning, and its      tropy maximization and stability. Our theoretical results
-natural interpretation as the inverse of the temperature in      derive soft policy iteration, which we show to converge to
-the maximum entropy framework provides good intuition            the optimal policy. From this result, we can formulate a
-for how to adjust this parameter.                                soft actor-critic algorithm, and we empirically show that it
-                                                                 outperforms state-of-the-art model-free deep RL methods,
-Target network update. It is common to use a separate            including the off-policy DDPG algorithm and the on-policy
-target value network that slowly tracks the actual value func-   PPO algorithm. In fact, the sample efﬁciency of this ap-
-tion to improve stability. We use an exponentially moving        proach actually exceeds that of DDPG by a substantial mar-
-average, with a smoothing constant τ , to update the target      gin. Our results suggest that stochastic, entropy maximizing
-value network weights as common in the prior work (Lill-         reinforcement learning algorithms can provide a promising
-icrap et al., 2015; Mnih et al., 2015). A value of one cor-      avenue for improved robustness and stability, and further
-responds to a hard update where the weights are copied           exploration of maximum entropy methods, including meth-
-directly at every iteration and zero to not updating the target  ods that incorporate second order information (e.g., trust
-at all. In Figure 3(c), we compare the performance of SAC        regions (Schulman et al., 2015)) or more expressive policy
-when τ varies. Large τ can lead to instabilities while small     classes is an exciting avenue for future work.
-τ can make training slower. However, we found the range
-of suitable values of τ to be relatively wide and we used        Acknowledgments
-the same value (0.005) across all of the tasks. In Figure 4
-(Appendix E) we also compare to another variant of SAC,          We would like to thank Vitchyr Pong for insightful discus-
-where instead of using exponentially moving average, we          sions and help in implementing our algorithm as well as
-copy over the current network weights directly into the tar-     providing the DDPG baseline code; Oﬁr Nachum for offer-
-get network every 1000 gradient steps. We found this variant     ing support in running Trust-PCL experiments; and George
-to beneﬁt from taking more than one gradient step between        Tucker for his valuable feedback on an early version of this
-the environment steps, which can improve performance but         paper. This work was supported by Siemens and Berkeley
-also increases the computational cost.                           DeepDrive.
-Soft Actor-Critic
-
-References                                                      Levine, S. and Koltun, V. Guided policy search. In Interna-
-                                                                   tional Conference on Machine Learning (ICML), pp. 1–9,
-Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike        2013.
-   adaptive elements that can solve difﬁcult learning con-
-   trol problems. IEEE transactions on systems, man, and        Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end
-   cybernetics, pp. 834–846, 1983.                                 training of deep visuomotor policies. Journal of Machine
-                                                                  Learning Research, 17(39):1–40, 2016.
-Bhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,
-   H. R., and Szepesva´ri, C. Convergent temporal-difference    Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
-   learning with arbitrary smooth function approximation.         T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
-   In Advances in Neural Information Processing Systems            control with deep reinforcement learning. arXiv preprint
-  (NIPS), pp. 1204–1212, 2009.                                     arXiv:1509.02971, 2015.
-
-Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,        Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
-   Schulman, J., Tang, J., and Zaremba, W. OpenAI gym.            Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing
-   arXiv preprint arXiv:1606.01540, 2016.                          atari with deep reinforcement learning. arXiv preprint
-                                                                   arXiv:1312.5602, 2013.
-Duan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel,
-   P. Benchmarking deep reinforcement learning for contin-      Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
-   uous control. In International Conference on Machine           J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
-  Learning (ICML), 2016.                                           land, A. K., Ostrovski, G., et al. Human-level control
-                                                                   through deep reinforcement learning. Nature, 518(7540):
-Fox, R., Pakman, A., and Tishby, N. Taming the noise in            529–533, 2015.
-   reinforcement learning via soft updates. In Conference
-   on Uncertainty in Artiﬁcial Intelligence (UAI), 2016.        Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
-                                                                  T. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
-Fujimoto, S., van Hoof, H., and Meger, D. Addressing func-         chronous methods for deep reinforcement learning. In
-   tion approximation error in actor-critic methods. arXiv        International Conference on Machine Learning (ICML),
-   preprint arXiv:1802.09477, 2018.                                2016.
-
-Gruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R.       Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
-  The reactor: A sample-efﬁcient actor-critic architecture.        Bridging the gap between value and policy based rein-
-   arXiv preprint arXiv:1704.04651, 2017.                          forcement learning. In Advances in Neural Information
-                                                                  Processing Systems (NIPS), pp. 2772–2782, 2017a.
-Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and
-   Levine, S. Q-prop: Sample-efﬁcient policy gradient with      Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
-   an off-policy critic. arXiv preprint arXiv:1611.02247,         Trust-PCL: An off-policy trust region method for contin-
-   2016.                                                           uous control. arXiv preprint arXiv:1707.01891, 2017b.
-
-Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-        O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.
-   forcement learning with deep energy-based policies. In          PGQ: Combining policy gradient and Q-learning. arXiv
-  International Conference on Machine Learning (ICML),             preprint arXiv:1611.01626, 2016.
-   pp. 1352–1361, 2017.
-                                                                Peters, J. and Schaal, S. Reinforcement learning of motor
-Hasselt, H. V. Double Q-learning. In Advances in Neural            skills with policy gradients. Neural networks, 21(4):682–
-  Information Processing Systems (NIPS), pp. 2613–2621,            697, 2008.
-   2010.
-                                                                Rawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-
-Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and     tic optimal control and reinforcement learning by approx-
-  Tassa, Y. Learning continuous control policies by stochas-       imate inference. Robotics: Science and Systems (RSS),
-   tic value gradients. In Advances in Neural Information          2012.
-  Processing Systems (NIPS), pp. 2944–2952, 2015.
-                                                                Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., and
-Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup,         Moritz, P. Trust region policy optimization. In Inter-
-   D., and Meger, D. Deep reinforcement learning that              national Conference on Machine Learning (ICML), pp.
-   matters. arXiv preprint arXiv:1709.06560, 2017.                1889–1897, 2015.
-
-Kingma, D. and Ba, J. Adam: A method for stochastic
-   optimization. In International Conference for Learning
-  Presentations (ICLR), 2015.
-                                                                       Soft Actor-Critic
-
-Schulman, J., Abbeel, P., and Chen, X. Equivalence be-
-   tween policy gradients and soft Q-learning. arXiv preprint
-   arXiv:1704.06440, 2017a.
-
-Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
-   Klimov, O. Proximal policy optimization algorithms.
-   arXiv preprint arXiv:1707.06347, 2017b.
-
-Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,
-   and Riedmiller, M. Deterministic policy gradient algo-
-   rithms. In International Conference on Machine Learning
-  (ICML), 2014.
-
-Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
-  van den Driessche, G., Schrittwieser, J., Antonoglou, I.,
-   Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe,
-   D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,
-   Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,
-   D. Mastering the game of go with deep neural networks
-   and tree search. Nature, 529(7587):484–489, Jan 2016.
-   ISSN 0028-0836. Article.
-
-Sutton, R. S. and Barto, A. G. Reinforcement learning: An
-   introduction, volume 1. MIT press Cambridge, 1998.
-
-Thomas, P. Bias in natural actor-critic algorithms. In Inter-
-   national Conference on Machine Learning (ICML), pp.
-  441–448, 2014.
-
-Todorov, E. General duality between optimal control and
-   estimation. In IEEE Conference on Decision and Control
-  (CDC), pp. 4286–4292. IEEE, 2008.
-
-Toussaint, M. Robot trajectory optimization using approxi-
-   mate inference. In International Conference on Machine
-  Learning (ICML), pp. 1049–1056. ACM, 2009.
-
-Williams, R. J. Simple statistical gradient-following algo-
-   rithms for connectionist reinforcement learning. Machine
-   learning, 8(3-4):229–256, 1992.
-
-Ziebart, B. D. Modeling purposeful adaptive behavior with
-   the principle of maximum causal entropy. Carnegie Mel-
-   lon University, 2010.
-
-Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.
-   Maximum entropy inverse reinforcement learning. In
-  AAAI Conference on Artiﬁcial Intelligence (AAAI), pp.
-  1433–1438, 2008.
-                                                Soft Actor-Critic
-
-A. Maximum Entropy Objective
-
-The exact deﬁnition of the discounted maximum entropy objective is complicated by the fact that, when using a discount
-factor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,
-discounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,
-with the discount serving to reduce variance, as discussed by Thomas (2014). However, we can deﬁne the objective that is
-optimized under a discount factor as
-
-        ∞                                  ∞
-
-J(π) =       E(st ,at )∼ρπ                      γl−t Esl∼p,al∼π [r(st, at) + αH(π( · |st))|st, at] .  (14)
-
-        t=0                                l=t
-
-This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from
-every state-action tuple (st, at) weighted by its probability ρπ under the current policy.
-
-B. Proofs
-
-B.1. Lemma 1
-
-Lemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T π in Equation 2 and a mapping
-Q0 : S × A → R with |A| < ∞, and deﬁne Qk+1 = T πQk. Then the sequence Qk will converge to the soft Q-value of π
-as k → ∞.
-
-Proof. Deﬁne the entropy augmented reward as rπ(st, at) r(st, at) + Est+1∼p [H (π( · |st+1))] and rewrite the update
-rule as
-
-        Q(st, at) ← rπ(st, at) + γ Est+1∼p,at+1∼π [Q(st+1, at+1)]                                     (15)
-
-and apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < ∞ is
-
-required to guarantee that the entropy augmented reward is bounded.
-
-B.2. Lemma 2
-
-Lemma 2 (Soft Policy Improvement). Let πold ∈ Π and let πnew be the optimizer of the minimization problem deﬁned in
-Equation 4. Then Qπnew (st, at) ≥ Qπold (st, at) for all (st, at) ∈ S × A with |A| < ∞.
-
-Proof. Let πold ∈ Π and let Qπold and V πold be the corresponding soft state-action value and soft state value, and let πnew
-be deﬁned as
-
-πnew( · |st) = arg min DKL (π ( · |st) exp (Qπold (st, · ) − log Zπold (st)))
-
-                               π ∈Π
-
-             =  arg  min                   Jπold (π  (  ·  |st)).                                     (16)
-
-                     π ∈Π
-
-It must be the case that Jπold (πnew( · |st)) ≤ Jπold (πold( · |st)), since we can always choose πnew = πold ∈ Π. Hence
-
-Eat∼πnew [log πnew(at|st) − Qπold (st, at) + log Zπold (st)] ≤ Eat∼πold [log πold(at|st) − Qπold (st, at) + log Zπold (st)],
-                                                                                                                                              (17)
-
-and since partition function Zπold depends only on the state, the inequality reduces to
-
-             Eat∼πnew [Qπold (st, at) − log πnew(at|st)] ≥ V πold (st).                               (18)
-
-Next, consider the soft Bellman equation:
-
-Qπold (st, at) = r(st, at) + γ Est+1∼p [V πold (st+1)]                                                (19)
-                ≤ r(st, at) + γ Est+1∼p Eat+1∼πnew [Qπold (st+1, at+1) − log πnew(at+1|st+1)]
-                 ...
-                ≤ Qπnew (st, at),
-
-where we have repeatedly expanded Qπold on the RHS by applying the soft Bellman equation and the bound in Equation 18.
-Convergence to Qπnew follows from Lemma 1.
-                                                                       Soft Actor-Critic
-
-B.3. Theorem 1
-
-Theorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any π ∈ Π
-converges to a policy π∗ such that Qπ∗ (st, at) ≥ Qπ(st, at) for all π ∈ Π and (st, at) ∈ S × A, assuming |A| < ∞.
-
-Proof. Let πi be the policy at iteration i. By Lemma 2, the sequence Qπi is monotonically increasing. Since Qπ is bounded
-above for π ∈ Π (both the reward and entropy are bounded), the sequence converges to some π∗. We will still need to
-show that π∗ is indeed optimal. At convergence, it must be case that Jπ∗ (π∗( · |st)) < Jπ∗ (π( · |st)) for all π ∈ Π, π = π∗.
-Using the same iterative argument as in the proof of Lemma 2, we get Qπ∗ (st, at) > Qπ(st, at) for all (st, at) ∈ S × A,
-that is, the soft value of any other policy in Π is lower than that of the converged policy. Hence π∗ is optimal in Π.
-
-C. Enforcing Action Bounds
-
-We use an unbounded Gaussian as the action distribution. However, in practice, the actions needs to be bounded to a ﬁnite
-
-interval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of
-variables formula to compute the likelihoods of the bounded actions. In the other words, let u ∈ RD be a random variable
-and µ(u|s) the corresponding density with inﬁnite support. Then a = tanh(u), where tanh is applied elementwise, is a
-random variable with support in (−1, 1) with a density given by
-
-                                   π(a|s) = µ(u|s) det  da  −1                                   (20)
-                                                        du
-                                                               .
-
-Since the Jacobian da/du = diag(1 − tanh2(u)) is diagonal, the log-likelihood has a simple form
-
-                                   D
-
-                                   log π(a|s) = log µ(u|s) − log 1 − tanh2(ui) ,                 (21)
-
-                                   i=1
-
-where ui is the ith element of u.
-                                                                       Soft Actor-Critic
-
-D. Hyperparameters
-
-Table 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the
-reward scale parameter that was tuned for each environment.
-
-             Table 1. SAC Hyperparameters
-
-Parameter                                  Value
-
-Shared                                     Adam (Kingma & Ba, 2015)
-   optimizer                               3 · 10−4
-   learning rate                           0.99
-   discount (γ)                            106
-   replay buffer size                      2
-   number of hidden layers (all networks)  256
-   number of hidden units per layer        256
-   number of samples per minibatch         ReLU
-   nonlinearity
-
-SAC                                        0.005
-   target smoothing coefﬁcient (τ )        1
-   target update interval                  1
-   gradient steps
-
-SAC (hard target update)                   1
-   target smoothing coefﬁcient (τ )        1000
-   target update interval                  4
-   gradient steps (except humanoids)       1
-   gradient steps (humanoids)
-
-           Table 2. SAC Environment Speciﬁc Parameters
-
-Environment  Action Dimensions Reward Scale
-
-Hopper-v1    3                                    5
-
-Walker2d-v1  6                                    5
-
-HalfCheetah-v1 6                                  5
-
-Ant-v1       8                                    5
-
-Humanoid-v1 17                                    20
-
-Humanoid (rllab) 21                               10
-                                                                                          Soft Actor-Critic
-
-E. Additional Baseline Results
-
-Figure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of
-environment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The ﬁgure also
-includes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using
-exponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update
-(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two
-Q-functions, using hard target updates, not having a separate target actor, and using ﬁxed exploration noise rather than
-learned. Both of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)
-task, on which SAC is the fastest.
-
-                                          Hopper-v1                                                     Walker2d-v1                                          HalfCheetah-v1
-
-average return  4000                                           average return  6000                                            average return  15000
-                3000                                                           5000                                                            10000
-                2000                                                           4000
-                                                                               3000
-
-                1000                                                           2000                                                            5000
-
-                                                                               1000
-
-                0                                                              0                                                                    0
-                                                                                                                                                     0.0
-                      0.0  0.2  0.4  0.6             0.8  1.0                        0.0  0.2  0.4  0.6              0.8  1.0                             0.5 1.0 1.5 2.0 2.5 3.0
-                                million steps                                                  million steps                                   6000                    million steps
-                                                                                                                                               4000
-                           (a) Hopper-v1                                                  (b) Walker2d-v1                                                 (c) HalfCheetah-v1
-
-                                     Ant-v1                                                       Humanoid-v1                                                     Humanoid (rllab)
-
-                6000                                                           8000                                                                       SAC
-                                                                                                                                                          SAC (hard target update)
-average return  4000                                           average return  6000                                            average return             SAC (hard target update, deterministic)
-                                                                                                                                                          Trust-PCL
-
-                                                                               4000
-
-                2000                                                                                                                           2000
-
-                                                                               2000
-
-                0                                                              0                                                               0
-                 0.0 0.5 1.0 1.5 2.0 2.5 3.0
-                                       million steps                                 0    2    4    6                8    10                          0   2  4  6               8  10
-                                                                                               million steps                                                 million steps
-                            (d) Ant-v1
-                                                                                          (e) Humanoid-v1                                                 (f) Humanoid (rllab)
-
-Figure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue)
-differs from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially
-smoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with ﬁxed Gaussian exploration noise,
-does not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target
-Q-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.
-
diff --git a/reference papers/TD3_paper.pdf b/reference papers/TD3_paper.pdf
deleted file mode 100644
index 8019838..0000000
--- a/reference papers/TD3_paper.pdf	
+++ /dev/null
@@ -1,648 +0,0 @@
-Addressing Function Approximation Error in Actor-Critic Methods
-
-Scott Fujimoto 1 Herke van Hoof 2 David Meger 1
-
-                        Abstract                                 means using an imprecise estimate within each update will
-                                                                 lead to an accumulation of error. Due to overestimation bias,
-      In value-based reinforcement learning methods              this accumulated error can cause arbitrarily bad states to
-      such as deep Q-learning, function approximation            be estimated as high value, resulting in suboptimal policy
-      errors are known to lead to overestimated value            updates and divergent behavior.
-      estimates and suboptimal policies. We show that
-      this problem persists in an actor-critic setting and       This paper begins by establishing this overestimation prop-
-      propose novel mechanisms to minimize its effects           erty is also present for deterministic policy gradients (Silver
-      on both the actor and the critic. Our algorithm            et al., 2014), in the continuous control setting. Furthermore,
-      builds on Double Q-learning, by taking the mini-           we ﬁnd the ubiquitous solution in the discrete action setting,
-      mum value between a pair of critics to limit over-         Double DQN (Van Hasselt et al., 2016), to be ineffective
-      estimation. We draw the connection between tar-            in an actor-critic setting. During training, Double DQN
-      get networks and overestimation bias, and suggest          estimates the value of the current policy with a separate tar-
-      delaying policy updates to reduce per-update error         get value function, allowing actions to be evaluated without
-      and further improve performance. We evaluate               maximization bias. Unfortunately, due to the slow-changing
-      our method on the suite of OpenAI gym tasks,               policy in an actor-critic setting, the current and target value
-      outperforming the state of the art in every envi-          estimates remain too similar to avoid maximization bias.
-      ronment tested.                                            This can be dealt with by adapting an older variant, Double
-                                                                 Q-learning (Van Hasselt, 2010), to an actor-critic format
-1. Introduction                                                  by using a pair of independently trained critics. While this
-                                                                 allows for a less biased value estimation, even an unbiased
-In reinforcement learning problems with discrete action          estimate with high variance can still lead to future overes-
-spaces, the issue of value overestimation as a result of func-   timations in local regions of state space, which in turn can
-tion approximation errors is well-studied. However, similar      negatively affect the global policy. To address this concern,
-issues with actor-critic methods in continuous control do-       we propose a clipped Double Q-learning variant which lever-
-mains have been largely left untouched. In this paper, we        ages the notion that a value estimate suffering from overes-
-show overestimation bias and the accumulation of error in        timation bias can be used as an approximate upper-bound to
-temporal difference methods are present in an actor-critic       the true value estimate. This favors underestimations, which
-setting. Our proposed method addresses these issues, and         do not tend to be propagated during learning, as actions with
-greatly outperforms the current state of the art.                low value estimates are avoided by the policy.
-
-Overestimation bias is a property of Q-learning in which the     Given the connection of noise to overestimation bias, this
-maximization of a noisy value estimate induces a consistent      paper contains a number of components that address vari-
-overestimation (Thrun & Schwartz, 1993). In a function           ance reduction. First, we show that target networks, a com-
-approximation setting, this noise is unavoidable given the       mon approach in deep Q-learning methods, are critical for
-imprecision of the estimator. This inaccuracy is further         variance reduction by reducing the accumulation of errors.
-exaggerated by the nature of temporal difference learning        Second, to address the coupling of value and policy, we
-(Sutton, 1988), in which an estimate of the value function       propose delaying policy updates until the value estimate
-is updated using the estimate of a subsequent state. This        has converged. Finally, we introduce a novel regularization
-                                                                 strategy, where a SARSA-style update bootstraps similar
-    1McGill University, Montreal, Canada 2University of Amster-  action estimates to further reduce variance.
-dam, Amsterdam, Netherlands. Correspondence to: Scott Fujimoto
-<scott.fujimoto@mail.mcgill.ca>.                                 Our modiﬁcations are applied to the state of the art actor-
-                                                                 critic method for continuous control, Deep Deterministic
-Proceedings of the 35 th International Conference on Machine     Policy Gradient algorithm (DDPG) (Lillicrap et al., 2015), to
-Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018       form the Twin Delayed Deep Deterministic policy gradient
-by the author(s).
-Addressing Function Approximation Error in Actor-Critic Methods
-
-algorithm (TD3), an actor-critic algorithm which consid-        horizon. Another approach is a reduction in the discount
-ers the interplay between function approximation error in       factor (Petrik & Scherrer, 2009), reducing the contribution
-both policy and value updates. We evaluate our algorithm        of each error.
-on seven continuous control domains from OpenAI gym
-(Brockman et al., 2016), where we outperform the state of       Our method builds on the Deterministic Policy Gradient
-the art by a wide margin.                                       algorithm (DPG) (Silver et al., 2014), an actor-critic method
-                                                                which uses a learned value estimate to train a deterministic
-Given the recent concerns in reproducibility (Henderson         policy. An extension of DPG to deep reinforcement learn-
-et al., 2017), we run our experiments across a large num-       ing, DDPG (Lillicrap et al., 2015), has shown to produce
-ber of seeds with fair evaluation metrics, perform abla-        state of the art results with an efﬁcient number of iterations.
-tion studies across each contribution, and open source both     Orthogonal to our approach, recent improvements to DDPG
-our code and learning curves (https://github.com/               include distributed methods (Popov et al., 2017), along with
-sfujim/TD3).                                                    multi-step returns and prioritized experience replay (Schaul
-                                                                et al., 2016; Horgan et al., 2018), and distributional methods
-2. Related Work                                                 (Bellemare et al., 2017; Barth-Maron et al., 2018).
-
-Function approximation error and its effect on bias and         3. Background
-variance in reinforcement learning algorithms have been
-studied in prior works (Pendrith et al., 1997; Mannor et al.,   Reinforcement learning considers the paradigm of an agent
-2007). Our work focuses on two outcomes that occur as the
-result of estimation error, namely overestimation bias and a    interacting with its environment with the aim of learning
-high variance build-up.
-                                                                reward-maximizing behavior. At each discrete time step
-Several approaches exist to reduce the effects of overestima-
-tion bias due to function approximation and policy optimiza-    t, with a given state s ∈ S, the agent selects actions
-tion in Q-learning. Double Q-learning uses two independent
-estimators to make unbiased value estimates (Van Hasselt,       a ∈ A with respect to its policy π : S → A, receiv-
-2010; Van Hasselt et al., 2016). Other approaches have
-focused directly on reducing the variance (Anschel et al.,      ing a reward r and the new state of the environment s .
-2017), minimizing over-ﬁtting to early high variance esti-
-mates (Fox et al., 2016), or through corrective terms (Lee      The return is deﬁned as the discounted sum of rewards
-et al., 2013). Further, the variance of the value estimate
-has been considered directly for risk-aversion (Mannor &        Rt =  T    γi−tr(si,  ai),  where  γ  is  a  discount  factor  de-
-Tsitsiklis, 2011) and exploration (O’Donoghue et al., 2017),          i=t
-but without connection to overestimation bias.
-                                                                termining the priority of short-term rewards.
-The concern of variance due to the accumulation of error in
-temporal difference learning has been largely dealt with by     In reinforcement learning, the objective is to ﬁnd the op-
-either minimizing the size of errors at each time step or mix-  timal policy πφ, with parameters φ, which maximizes the
-ing off-policy and Monte-Carlo returns. Our work shows          expected return J (φ) = Esi∼pπ,ai∼π [R0]. For continuous
-the importance of a standard technique, target networks, for    control, parametrized policies πφ can be updated by taking
-the reduction of per-update error, and develops a regulariza-   the gradient of the expected return ∇φJ(φ). In actor-critic
-tion technique for the variance reduction by averaging over     methods, the policy, known as the actor, can be updated
-value estimates. Concurrently, Nachum et al. (2018) showed
-smoothed value functions could be used to train stochastic      through the deterministic policy gradient algorithm (Silver
-policies with reduced variance and improved performance.
-Methods with multi-step returns offer a trade-off between       et al., 2014):
-accumulated estimation bias and variance induced by the
-policy and the environment. These methods have been                ∇φJ (φ) = Es∼pπ ∇aQπ(s, a)|a=π(s)∇φπφ(s) . (1)
-shown to be an effective approach, through importance sam-
-pling (Precup et al., 2001; Munos et al., 2016), distributed    Qπ(s, a) = Esi∼pπ,ai∼π [Rt|s, a], the expected return
-methods (Mnih et al., 2016; Espeholt et al., 2018), and ap-     when performing action a in state s and following π af-
-proximate bounds (He et al., 2016). However, rather than        ter, is known as the critic or the value function.
-provide a direct solution to the accumulation of error, these
-methods circumvent the problem by considering a longer          In Q-learning, the value function can be learned using tem-
-                                                                poral difference learning (Sutton, 1988; Watkins, 1989), an
-                                                                update rule based on the Bellman equation (Bellman, 1957).
-                                                                The Bellman equation is a fundamental relationship between
-                                                                the value of a state-action pair (s, a) and the value of the
-                                                                subsequent state-action pair (s , a ):
-
-                                                                  Qπ(s, a) = r + γEs ,a [Qπ(s , a )] , a ∼ π(s ). (2)
-
-                                                                For a large state space, the value can be estimated with a
-                                                                differentiable function approximator Qθ(s, a), with param-
-                                                                eters θ. In deep Q-learning (Mnih et al., 2015), the network
-                                                                is updated by using temporal difference learning with a sec-
-                                                                ondary frozen target network Qθ (s, a) to maintain a ﬁxed
-                        Addressing Function Approximation Error in Actor-Critic Methods
-
-objective y over multiple updates:                                              400                    500
-
-                                                                 Average Value  300                    400
-
-y = r + γQθ (s , a ), a ∼ πφ (s ),        (3)                                   200                    300
-
-                                                                                                       200
-
-where the actions are selected from a target actor network                      100  CDQ  True CDQ 100
-πφ . The weights of a target network are either updated
-periodically to exactly match the weights of the current                        0    DDPG True DDPG    0
-network, or by some proportion τ at each time step θ ←
-τ θ + (1 − τ )θ . This update can be applied in an off-policy                        0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
-fashion, sampling random mini-batches of transitions from                            Time steps (1e6)       Time steps (1e6)
-
-an experience replay buffer (Lin, 1992).                                             (a) Hopper-v1          (b) Walker2d-v1
-
-                                                                 Figure 1. Measuring overestimation bias in the value estimates
-                                                                 of DDPG and our proposed method, Clipped Double Q-learning
-                                                                 (CDQ), on MuJoCo environments over 1 million time steps.
-
-4. Overestimation Bias                                           occur with slightly stricter conditions. We examine this case
-                                                                 further in the supplementary material. We denote πapprox
-In Q-learning with discrete actions, the value estimate is       and πtrue as the policy with parameters φapprox and φtrue re-
-updated with a greedy target y = r + γ maxa Q(s , a ),           spectively.
-however, if the target is susceptible to error , then the max-
-imum over the value along with its error will generally be       As the gradient direction is a local maximizer, there exists 1
-greater than the true maximum, E [maxa (Q(s , a ) + )] ≥         sufﬁciently small such that if α ≤ 1 then the approximate
-maxa Q(s , a ) (Thrun & Schwartz, 1993). As a result,            value of πapprox will be bounded below by the approximate
-even initially zero-mean error can cause value updates to        value of πtrue:
-result in a consistent overestimation bias, which is then prop-
-agated through the Bellman equation. This is problematic as               E [Qθ(s, πapprox(s))] ≥ E [Qθ(s, πtrue(s))] . (5)
-errors induced by function approximation are unavoidable.
-                                                                 Conversely, there exists 2 sufﬁciently small such that if
-While in the discrete action setting overestimation bias is      α ≤ 2 then the true value of πapprox will be bounded above
-an obvious artifact from the analytical maximization, the        by the true value of πtrue:
-presence and effects of overestimation bias is less clear in an
-actor-critic setting where the policy is updated via gradient             E [Qπ(s, πtrue(s))] ≥ E [Qπ(s, πapprox(s))] . (6)
-descent. We begin by proving that the value estimate in de-
-terministic policy gradients will be an overestimation under     If in expectation the value estimate is at least as large as
-some basic assumptions in Section 4.1 and then propose           the true value with respect to φtrue, E [Qθ (s, πtrue(s))] ≥
-a clipped variant of Double Q-learning in an actor-critic        E [Qπ (s, πtrue(s))], then Equations (5) and (6) imply that if
-setting to reduce overestimation bias in Section 4.2.            α < min( 1, 2), then the value estimate will be overesti-
-                                                                 mated:
-4.1. Overestimation Bias in Actor-Critic
-                                                                         E [Qθ(s, πapprox(s))] ≥ E [Qπ(s, πapprox(s))] . (7)
-In actor-critic methods the policy is updated with respect
-to the value estimates of an approximate critic. In this         Although this overestimation may be minimal with each
-section we assume the policy is updated using the deter-         update, the presence of error raises two concerns. Firstly, the
-ministic policy gradient, and show that the update induces       overestimation may develop into a more signiﬁcant bias over
-overestimation in the value estimate. Given current policy       many updates if left unchecked. Secondly, an inaccurate
-parameters φ, let φapprox deﬁne the parameters from the ac-      value estimate may lead to poor policy updates. This is
-tor update induced by the maximization of the approximate        particularly problematic because a feedback loop is created,
-critic Qθ(s, a) and φtrue the parameters from the hypothet-      in which suboptimal actions might be highly rated by the
-ical actor update with respect to the true underlying value      suboptimal critic, reinforcing the suboptimal action in the
-function Qπ(s, a) (which is not known during learning):          next policy update.
-
-                  α     ∇φπφ(s)∇aQθ(s, a)|a=πφ(s)                Does this theoretical overestimation occur in practice
-φapprox = φ + Z1 Es∼pπ                                           for state-of-the-art methods? We answer this question by
-                                                                 plotting the value estimate of DDPG (Lillicrap et al., 2015)
-               α        ∇φπφ(s)∇aQπ(s, a)|a=πφ(s)  ,             over time while it learns on the OpenAI gym environments
-φtrue = φ + Z2 Es∼pπ                                             Hopper-v1 and Walker2d-v1 (Brockman et al., 2016). In
-                                          (4)                    Figure 1, we graph the average value estimate over 10000
-                                                                 states and compare it to an estimate of the true value. The
-where we assume Z1 and Z2 are chosen to normalize the
-gradient, i.e., such that Z−1||E[·]|| = 1. Without normal-
-
-ized gradients, overestimation bias is still guaranteed to
-                                      Addressing Function Approximation Error in Actor-Critic Methods
-
-Average Value  400                                                                                         demonstrates that the actor-critic Double DQN suffers from
-                                                                               400                         a similar overestimation as DDPG (as shown in Figure 1).
-                                                                                                           While Double Q-learning is more effective, it does not en-
-               300                                                                                         tirely eliminate the overestimation. We further show this
-                                                                               300                         reduction is not sufﬁcient experimentally in Section 6.1.
-
-               200                    200                                                                  As πφ1 optimizes with respect to Qθ1 , using an indepen-
-                                                                                                           dent estimate in the target update of Qθ1 would avoid the
-               100  DQ-AC  True DQ-AC 100                                                                  bias introduced by the policy update. However the critics
-                                                                                                           are not entirely independent, due to the use of the oppo-
-               0    DDQN-AC True DDQN-AC 0                                                                 site critic in the learning targets, as well as the same re-
-                    0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0                                        play buffer. As a result, for some states s we will have
-                    Time steps (1e6)                                                Time steps (1e6)       Qθ2 (s, πφ1 (s)) > Qθ1 (s, πφ1 (s)). This is problematic be-
-                                                                                                           cause Qθ1 (s, πφ1 (s)) will generally overestimate the true
-                    (a) Hopper-v1                                                   (b) Walker2d-v1        value, and in certain areas of the state space the overestima-
-                                                                                                           tion will be further exaggerated. To address this problem,
-Figure 2. Measuring overestimation bias in the value estimates of                                          we propose to simply upper-bound the less biased value
-actor critic variants of Double DQN (DDQN-AC) and Double Q-                                                estimate Qθ2 by the biased estimate Qθ1 . This results in
-learning (DQ-AC) on MuJoCo environments over 1 million time                                                taking the minimum between the two estimates, to give the
-steps.                                                                                                     target update of our Clipped Double Q-learning algorithm:
-
-true value is estimated using the average discounted return                                                y1  =  r  +  γ  min    Qθi (s  ,  πφ1 (s  )).  (10)
-over 1000 episodes following the current policy, starting
-from states sampled from the replay buffer. A very clear                                                                   i=1,2
-overestimation bias occurs from the learning procedure,
-which contrasts with the novel method that we describe in
-the following section, Clipped Double Q-learning, which
-greatly reduces overestimation by the critic.
-
-4.2. Clipped Double Q-Learning for Actor-Critic                                                            With Clipped Double Q-learning, the value target cannot
-                                                                                                           introduce any additional overestimation over using the stan-
-While several approaches to reducing overestimation bias                                                   dard Q-learning target. While this update rule may induce
-have been proposed, we ﬁnd them ineffective in an actor-                                                   an underestimation bias, this is far preferable to overesti-
-critic setting. This section introduces a novel clipped variant                                            mation bias, as unlike overestimated actions, the value of
-of Double Q-learning (Van Hasselt, 2010), which can re-                                                    underestimated actions will not be explicitly propagated
-place the critic in any actor-critic method.                                                               through the policy update.
-
-In Double Q-learning, the greedy update is disentangled                                                    In implementation, computational costs can be reduced by
-from the value function by maintaining two separate value                                                  using a single actor optimized with respect to Qθ1 . We then
-estimates, each of which is used to update the other. If the                                               use the same target y2 = y1 for Qθ2 . If Qθ2 > Qθ1 then
-value estimates are independent, they can be used to make                                                  the update is identical to the standard update and induces no
-unbiased estimates of the actions selected using the opposite                                              additional bias. If Qθ2 < Qθ1 , this suggests overestimation
-value estimate. In Double DQN (Van Hasselt et al., 2016),                                                  has occurred and the value is reduced similar to Double Q-
-the authors propose using the target network as one of the                                                 learning. A proof of convergence in the ﬁnite MDP setting
-value estimates, and obtain a policy by greedy maximization                                                follows from this intuition. We provide formal details and
-of the current value network rather than the target network.                                               justiﬁcation in the supplementary material.
-In an actor-critic setting, an analogous update uses the cur-
-rent policy rather than the target policy in the learning target:                                          A secondary beneﬁt is that by treating the function approxi-
-                                                                                                           mation error as a random variable we can see that the min-
-                           y = r + γQθ (s , πφ(s )).                                                  (8)  imum operator should provide higher value to states with
-                                                                                                           lower variance estimation error, as the expected minimum
-In practice however, we found that with the slow-changing                                                  of a set of random variables decreases as the variance of
-policy in actor-critic, the current and target networks were                                               the random variables increases. This effect means that the
-too similar to make an independent estimation, and offered                                                 minimization in Equation (10) will lead to a preference for
-little improvement. Instead, the original Double Q-learning                                                states with low-variance value estimates, leading to safer
-formulation can be used, with a pair of actors (πφ1 , πφ2 )                                                policy updates with stable learning targets.
-and critics (Qθ1 , Qθ2 ), where πφ1 is optimized with respect
-to Qθ1 and πφ2 with respect to Qθ2 :                                                                       5. Addressing Variance
-
-                           y1 = r + γQθ2 (s , πφ1 (s ))                                               (9)  While Section 4 deals with the contribution of variance to
-                                                                                                           overestimation bias, we also argue that variance itself should
-                           y2 = r + γQθ1 (s , πφ2 (s )).                                                   be directly addressed. Besides the impact on overestimation
-
-We measure the overestimation bias in Figure 2, which
-               Addressing Function Approximation Error in Actor-Critic Methods
-
-bias, high variance estimates provide a noisy gradient for the                 350                        104
-policy update. This is known to reduce learning speed (Sut-                    300
-ton & Barto, 1998) as well as hurt performance in practice.     Average Value  250                        103
-In this section we emphasize the importance of minimizing                      200
-error at each update, build the connection between target                      150 0.0  τ =1     τ = 0.01
-networks and estimation error and propose modiﬁcations to                               τ = 0.1  True Value 102
-the learning procedure of actor-critic for variance reduction.
-                                                                                        0.2 0.4 0.6 0.8 1.0 0.0  0.2 0.4 0.6 0.8 1.0
-                                                                                              Time steps (1e5)          Time steps (1e5)
-
-                                                                                        (a) Fixed Policy         (b) Learned Policy
-
-5.1. Accumulating Error                                         Figure 3. Average estimated value of a randomly selected state
-                                                                on Hopper-v1 without target networks, (τ = 1), and with slow-
-Due to the temporal difference update, where an estimate of     updating target networks, (τ = 0.1, 0.01), with a ﬁxed and a
-the value function is built from an estimate of a subsequent    learned policy.
-state, there is a build up of error. While it is reasonable to
-expect small error for an individual update, these estimation   procedure, and allow a greater coverage of the training data.
-errors can accumulate, resulting in the potential for large     Without a ﬁxed target, each update may leave residual error
-overestimation bias and suboptimal policy updates. This is      which will begin to accumulate. While the accumulation of
-exacerbated in a function approximation setting where the       error can be detrimental in itself, when paired with a policy
-Bellman equation is never exactly satisﬁed, and each update     maximizing over the value estimate, it can result in wildly
-leaves some amount of residual TD-error δ(s, a):                divergent values.
-
-Qθ(s, a) = r + γE[Qθ(s , a )] − δ(s, a). (11)                   To provide some intuition, we examine the learning behavior
-                                                                with and without target networks on both the critic and actor
-It can then be shown that rather than learning an estimate      in Figure 3, where we graph the value, in a similar manner to
-of the expected return, the value estimate approximates the     Figure 1, in the Hopper-v1 environment. In (a) we compare
-expected return minus the expected discounted sum of future     the behavior with a ﬁxed policy and in (b) we examine the
-TD-errors:                                                      value estimates with a policy that continues to learn, trained
-                                                                with the current value estimate. The target networks use a
-Qθ(st, at) = rt + γE[Qθ(st+1, at+1)] − δt                       slow-moving update rate, parametrized by τ .
-
-= rt + γE [rt+1 + γE [Qθ(st+2, at+2) − δt+1]] − δt              While updating the value estimate without target networks
-                                                                (τ = 1) increases the volatility, all update rates result in sim-
-= Esi∼pπ,ai∼π   T                          (12)                 ilar convergent behaviors when considering a ﬁxed policy.
-                                                                However, when the policy is trained with the current value
-                   γi−t(ri − δi) .                              estimate, the use of fast-updating target networks results in
-                                                                highly divergent behavior.
-               i=t
-                                                                When do actor-critic methods fail to learn? These results
-If the value estimate is a function of future reward and es-    suggest that the divergence that occurs without target net-
-timation error, it follows that the variance of the estimate    works is the result of policy updates with a high variance
-will be proportional to the variance of future reward and es-   value estimate. Figure 3, as well as Section 4, suggest failure
-timation error. Given a large discount factor γ, the variance   can occur due to the interplay between the actor and critic
-can grow rapidly with each update if the error from each        updates. Value estimates diverge through overestimation
-update is not tamed. Furthermore each gradient update only      when the policy is poor, and the policy will become poor if
-reduces error with respect to a small mini-batch which gives    the value estimate itself is inaccurate.
-no guarantees about the size of errors in value estimates
-outside the mini-batch.                                         If target networks can be used to reduce the error over mul-
-                                                                tiple updates, and policy updates on high-error states cause
-5.2. Target Networks and Delayed Policy Updates                 divergent behavior, then the policy network should be up-
-                                                                dated at a lower frequency than the value network, to ﬁrst
-In this section we examine the relationship between target      minimize error before introducing a policy update. We pro-
-networks and function approximation error, and show the         pose delaying policy updates until the value error is as small
-use of a stable target reduces the growth of error. This        as possible. The modiﬁcation is to only update the policy
-insight allows us to consider the interplay between high        and target networks after a ﬁxed number of updates d to the
-variance estimates and policy performance, when designing       critic. To ensure the TD-error remains small, we update the
-reinforcement learning algorithms.
-
-Target networks are a well-known tool to achieve stabil-
-ity in deep reinforcement learning. As deep function ap-
-proximators require multiple gradient updates to converge,
-target networks provide a stable objective in the learning
-Addressing Function Approximation Error in Actor-Critic Methods
-
-target networks slowly θ ← τ θ + (1 − τ )θ .                      Algorithm 1 TD3
-
-By sufﬁciently delaying the policy updates we limit the like-        Initialize critic networks Qθ1 , Qθ2 , and actor network πφ
-lihood of repeating updates with respect to an unchanged             with random parameters θ1, θ2, φ
-critic. The less frequent policy updates that do occur will          Initialize target networks θ1 ← θ1, θ2 ← θ2, φ ← φ
-use a value estimate with lower variance, and in principle,          Initialize replay buffer B
-should result in higher quality policy updates. This creates a       for t = 1 to T do
-two-timescale algorithm, as often required for convergence
-in the linear setting (Konda & Tsitsiklis, 2003). The effec-            Select action with exploration noise a ∼ π(s) + ,
-tiveness of this strategy is captured by our empirical results            ∼ N (0, σ) and observe reward r and new state s
-presented in Section 6.1, which show an improvement in
-performance while using fewer policy updates.                           Store transition tuple (s, a, r, s ) in B
-
-5.3. Target Policy Smoothing Regularization                             Sample mini-batch of N transitions (s, a, r, s ) from B
-                                                                        a˜ ← πφ (s) + , ∼ clip(N (0, σ˜), −c, c)
-A concern with deterministic policies is they can overﬁt                y ← r + γ mini=1,2 Qθi (s , a˜)
-to narrow peaks in the value estimate. When updating the                Update critics θi ← minθi N −1 (y − Qθi (s, a))2
-critic, a learning target using a deterministic policy is highly        if t mod d then
-susceptible to inaccuracies induced by function approxima-
-tion error, increasing the variance of the target. This induced            Update φ by the deterministic policy gradient:
-variance can be reduced through regularization. We intro-                  ∇φJ (φ) = N −1 ∇aQθ1 (s, a)|a=πφ(s)∇φπφ(s)
-duce a regularization strategy for deep value learning, target             Update target networks:
-policy smoothing, which mimics the learning update from                    θi ← τ θi + (1 − τ )θi
-SARSA (Sutton & Barto, 1998). Our approach enforces                        φ ← τ φ + (1 − τ )φ
-the notion that similar actions should have similar value.              end if
-While the function approximation does this implicitly, the           end for
-relationship between similar actions can be forced explicitly
-by modifying the training procedure. We propose that ﬁtting
-the value of a small area around the target action
-
-y = r + E [Qθ (s , πφ (s ) + )] ,            (13)
-
-would have the beneﬁt of smoothing the value estimate by          (a)              (b)                  (c)                (d)
-bootstrapping off of similar state-action value estimates. In
-practice, we can approximate this expectation over actions        Figure 4. Example MuJoCo environments (a) HalfCheetah-v1, (b)
-by adding a small amount of random noise to the target            Hopper-v1, (c) Walker2d-v1, (d) Ant-v1.
-policy and averaging over mini-batches. This makes our
-modiﬁed target update:                                            6. Experiments
-
-y = r + γQθ (s , πφ (s ) + ),                (14)                 We present the Twin Delayed Deep Deterministic policy
-                                                                  gradient algorithm (TD3), which builds on the Deep Deter-
-∼ clip(N (0, σ), −c, c),                                          ministic Policy Gradient algorithm (DDPG) (Lillicrap et al.,
-                                                                  2015) by applying the modiﬁcations described in Sections
-where the added noise is clipped to keep the target in a          4.2, 5.2 and 5.3 to increase the stability and performance
-small range. The outcome is an algorithm reminiscent of           with consideration of function approximation error. TD3
-Expected SARSA (Van Seijen et al., 2009), where the value         maintains a pair of critics along with a single actor. For each
-estimate is instead learned off-policy and the noise added to     time step, we update the pair of critics towards the minimum
-the target policy is chosen independently of the exploration      target value of actions selected by the target policy:
-policy. The value estimate learned is with respect to a noisy
-policy deﬁned by the parameter σ.
-
-Intuitively, it is known that policies derived from SARSA              y  =  r  +  γ  min    Qθi (s  ,  πφ   (s  )  +  ),
-value estimates tend to be safer, as they provide higher value
-to actions resistant to perturbations. Thus, this style of                            i=1,2                                     (15)
-update can additionally lead to improvement in stochastic
-domains with failure cases. A similar idea was introduced                 ∼ clip(N (0, σ), −c, c).
-concurrently by Nachum et al. (2018), smoothing over Qθ,
-rather than Qθ .                                                  Every d iterations, the policy is updated with respect to Qθ1
-                                                                  following the deterministic policy gradient algorithm (Silver
-                                                                  et al., 2014). TD3 is summarized in Algorithm 1.
-                                                           Addressing Function Approximation Error in Actor-Critic Methods
-
-                                                      TD3       DDPG              our DDPG                    PPO               TRPO              ACKTR               SAC
-
-                10000                                           3500                                          5000
-
-                8000                                            3000                                          4000                                         4000
-
-Average Return  6000                                            2500                                          3000                                         3000
-                4000                                            2000                                          2000                                         2000
-                                                                1500
-                                                                                                                                                              1000
-                2000                                            1000                                          1000                                                 0
-                     0
-                       0.0                                      500                                                                                    1.0 −1000 0.0
-
-                            0.2  0.4             0.6  0.8  1.0  0     0.0    0.2  0.4  0.6          0.8  1.0  0     0.0    0.2  0.4   0.6         0.8                 0.2  0.4  0.6          0.8  1.0
-
-                                 Time steps (1e6)                                 Time steps (1e6)                              Time steps (1e6)                           Time steps (1e6)
-
-                            (a) HalfCheetah-v1                               (b) Hopper-v1                                 (c) Walker2d-v1                                 (d) Ant-v1
-
-                                                 −4                               1000                                          10000
-
-                                 Average Return                                        900                                      8000
-
-                                                 −6                                    800                                      6000
-
-                                                 −8                                    700                                      4000
-
-                                                                                       600
-
-                                                 −10                                   500                                      2000
-
-                                                 −12 0.0 0.2 0.4 0.6 0.8 1.0           400                                            0
-                                                                                                                                        0.0 0.2 0.4 0.6 0.8 1.0
-                                                           Time steps (1e6)                 0.0 0.2 0.4 0.6 0.8 1.0                                      Time steps (1e6)
-                                                                                                         Time steps (1e6)
-
-                                                      (e) Reacher-v1                        (f) InvertedPendulum-v1 (g) InvertedDoublePendulum-v1
-
-Figure 5. Learning curves for the OpenAI gym continuous control tasks. The shaded region represents half a standard deviation of the
-average evaluation over 10 trials. Some graphs are cropped to display the interesting regions.
-
-Table 1. Max Average Return over 10 trials of 1 million time steps. Maximum value for each task is bolded. ± corresponds to a single
-standard deviation over trials.
-
-                            Environment                            TD3                      DDPG         Our DDPG                PPO              TRPO     ACKTR            SAC
-
-                            HalfCheetah                    9636.95 ± 859.065                3305.60        8577.29              1795.43            -15.57  1450.46         2347.19
-                            Hopper                         3564.07 ± 114.74                 2020.46        1860.02              2164.70           2471.30  2428.39         2996.66
-                            Walker2d                       4682.82 ± 539.64                 1843.85        3098.11              3317.69           2321.47  1216.70         1283.67
-                            Ant                            4372.44 ± 1000.33                1005.30         888.77              1083.20            -75.85  1821.94         655.35
-                            Reacher                                                                                                               -111.43
-                            InvPendulum                        -3.60 ± 0.56                  -6.51           -4.01               -6.18            985.40     -4.26          -4.44
-                            InvDoublePendulum                1000.00 ± 0.00                 1000.00        1000.00              1000.00           205.85   1000.00         1000.00
-                                                            9337.47 ± 14.96                 9355.52        8369.95              8977.94                    9081.92         8487.15
-
-6.1. Evaluation                                                                                               N (0, 0.2) to the actions chosen by the target actor network,
-                                                                                                              clipped to (−0.5, 0.5), delayed policy updates consists of
-To evaluate our algorithm, we measure its performance on                                                      only updating the actor and target critic network every d
-the suite of MuJoCo continuous control tasks (Todorov et al.,                                                 iterations, with d = 2. While a larger d would result in a
-2012), interfaced through OpenAI Gym (Brockman et al.,                                                        larger beneﬁt with respect to accumulating errors, for fair
-2016) (Figure 4). To allow for reproducible comparison, we                                                    comparison, the critics are only trained once per time step,
-use the original set of tasks from Brockman et al. (2016)                                                     and training the actor for too few iterations would cripple
-with no modiﬁcations to the environment or reward.                                                            learning. Both target networks are updated with τ = 0.005.
-
-For our implementation of DDPG (Lillicrap et al., 2015), we                                                   To remove the dependency on the initial parameters of the
-use a two layer feedforward neural network of 400 and 300                                                     policy we use a purely exploratory policy for the ﬁrst 10000
-hidden nodes respectively, with rectiﬁed linear units (ReLU)                                                  time steps of stable length environments (HalfCheetah-v1
-between each layer for both the actor and critic, and a ﬁnal                                                  and Ant-v1) and the ﬁrst 1000 time steps for the remaining
-tanh unit following the output of the actor. Unlike the orig-                                                 environments. Afterwards, we use an off-policy exploration
-inal DDPG, the critic receives both the state and action as                                                   strategy, adding Gaussian noise N (0, 0.1) to each action.
-input to the ﬁrst layer. Both network parameters are updated                                                  Unlike the original implementation of DDPG, we used un-
-using Adam (Kingma & Ba, 2014) with a learning rate of                                                        correlated noise for exploration as we found noise drawn
-10−3. After each time step, the networks are trained with a                                                   from the Ornstein-Uhlenbeck (Uhlenbeck & Ornstein, 1930)
-mini-batch of a 100 transitions, sampled uniformly from a                                                     process offered no performance beneﬁts.
-replay buffer containing the entire history of the agent.
-                                                                                                              Each task is run for 1 million time steps with evaluations
-The target policy smoothing is implemented by adding ∼                                                        every 5000 time steps, where each evaluation reports the
-Addressing Function Approximation Error in Actor-Critic Methods
-
-average reward over 10 episodes with no exploration noise.      Table 2. Average return over the last 10 evaluations over 10 trials
-Our results are reported over 10 random seeds of the Gym        of 1 million time steps, comparing ablation over delayed policy
-simulator and the network initialization.                       updates (DP), target policy smoothing (TPS), Clipped Double
-                                                                Q-learning (CDQ) and our architecture, hyper-parameters and
-We compare our algorithm against DDPG (Lillicrap et al.,        exploration (AHE). Maximum value for each task is bolded.
-2015) as well as the state of art policy gradient algorithms:
-PPO (Schulman et al., 2017), ACKTR (Wu et al., 2017)            Method     HCheetah  Hopper   Walker2d    Ant
-and TRPO (Schulman et al., 2015), as implemented by
-OpenAI’s baselines repository (Dhariwal et al., 2017), and      TD3         9532.99  3304.75   4565.24  4185.06
-SAC (Haarnoja et al., 2018), as implemented by the author’s     DDPG        3162.50  1731.94   1520.90  816.35
-GitHub1. Additionally, we compare our method with our           AHE         8401.02  1061.77   2362.13  564.07
-re-tuned version of DDPG, which includes all architecture
-and hyper-parameter modiﬁcations to DDPG without any            AHE + DP    7588.64  1465.11   2459.53  896.13
-of our proposed adjustments. A full comparison between          AHE + TPS   9023.40  907.56    2961.36  872.17
-our re-tuned version and the baselines DDPG is provided in      AHE + CDQ   6470.20  1134.14   3979.21  3818.71
-the supplementary material.
-                                                                TD3 - DP    9590.65  2407.42   4695.50  3754.26
-Our results are presented in Table 1 and learning curves in     TD3 - TPS   8987.69  2392.59   4033.67  4155.24
-Figure 5. TD3 matches or outperforms all other algorithms       TD3 - CDQ   9792.80  1837.32   2579.39  849.75
-in both ﬁnal performance and learning speed across all tasks.
-                                                                DQ-AC       9433.87  1773.71   3100.45  2445.97
-                                                                DDQN-AC    10306.90  2155.75   3116.81  1092.18
-
-6.2. Ablation Studies                                           outperforms both prior methods, this suggests that subdu-
-                                                                ing the overestimations from the unbiased estimator is an
-We perform ablation studies to understand the contribution      effective measure to improve performance.
-of each individual component: Clipped Double Q-learning
-(Section 4.2), delayed policy updates (Section 5.2) and target  7. Conclusion
-policy smoothing (Section 5.3). We present our results in
-Table 2 in which we compare the performance of removing         Overestimation has been identiﬁed as a key problem in
-each component from TD3 along with our modiﬁcations to          value-based methods. In this paper, we establish overesti-
-the architecture and hyper-parameters. Additional learning      mation bias is also problematic in actor-critic methods. We
-curves can be found in the supplementary material.              ﬁnd the common solutions for reducing overestimation bias
-                                                                in deep Q-learning with discrete actions are ineffective in an
-The signiﬁcance of each component varies task to task.          actor-critic setting, and develop a novel variant of Double
-While the addition of only a single component causes in-        Q-learning which limits possible overestimation. Our re-
-signiﬁcant improvement in most cases, the addition of com-      sults demonstrate that mitigating overestimation can greatly
-binations performs at a much higher level. The full algo-       improve the performance of modern algorithms.
-rithm outperforms every other combination in most tasks.
-Although the actor is trained for only half the number of       Due to the connection between noise and overestimation,
-iterations, the inclusion of delayed policy update generally    we examine the accumulation of errors from temporal dif-
-improves performance, while reducing training time.             ference learning. Our work investigates the importance of
-                                                                a standard technique in deep reinforcement learning, target
-We additionally compare the effectiveness of the actor-critic   networks, and examines their role in limiting errors from
-variants of Double Q-learning (Van Hasselt, 2010) and Dou-      imprecise function approximation and stochastic optimiza-
-ble DQN (Van Hasselt et al., 2016), denoted DQ-AC and           tion. Finally, we introduce a SARSA-style regularization
-DDQN-AC respectively, in Table 2. For fairness in com-          technique which modiﬁes the temporal difference target to
-parison, these methods also beneﬁted from delayed policy        bootstrap off similar state-action pairs.
-updates, target policy smoothing and use our architecture
-and hyper-parameters. Both methods were shown to reduce         Taken together, these improvements deﬁne our proposed
-overestimation bias less than Clipped Double Q-learning in      approach, the Twin Delayed Deep Deterministic policy gra-
-Section 4. This is reﬂected empirically, as both methods        dient algorithm (TD3), which greatly improves both the
-result in insigniﬁcant improvements over TD3 - CDQ, with        learning speed and performance of DDPG in a number of
-an exception in the Ant-v1 environment, which appears to        challenging tasks in the continuous control setting. Our
-beneﬁt greatly from any overestimation reduction. As the        algorithm exceeds the performance of numerous state of
-inclusion of Clipped Double Q-learning into our full method     the art algorithms. As our modiﬁcations are simple to im-
-                                                                plement, they can be easily added to any other actor-critic
-    1See the supplementary material for hyper-parameters and a  algorithm.
-discussion on the discrepancy in the reported results of SAC.
-Addressing Function Approximation Error in Actor-Critic Methods
-
-References                                                      Kingma, D. and Ba, J. Adam: A method for stochastic
-                                                                   optimization. arXiv preprint arXiv:1412.6980, 2014.
-Anschel, O., Baram, N., and Shimkin, N. Averaged-dqn:
-  Variance reduction and stabilization for deep reinforce-      Konda, V. R. and Tsitsiklis, J. N. On actor-critic algorithms.
-   ment learning. In International Conference on Machine          SIAM journal on Control and Optimization, 42(4):1143–
-  Learning, pp. 176–185, 2017.                                    1166, 2003.
-
-Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney,            Lee, D., Defourny, B., and Powell, W. B. Bias-corrected
-  W., Horgan, D., TB, D., Muldal, A., Heess, N., and Lil-          q-learning to control max-operator bias in q-learning.
-   licrap, T. Distributional policy gradients. International       In Adaptive Dynamic Programming And Reinforcement
-  Conference on Learning Representations, 2018.                   Learning (ADPRL), 2013 IEEE Symposium on, pp. 93–99.
-                                                                   IEEE, 2013.
-Bellemare, M. G., Dabney, W., and Munos, R. A distribu-
-   tional perspective on reinforcement learning. In Interna-    Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
-   tional Conference on Machine Learning, pp. 449–458,            T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
-   2017.                                                           control with deep reinforcement learning. arXiv preprint
-                                                                   arXiv:1509.02971, 2015.
-Bellman, R. Dynamic Programming. Princeton University
-   Press, 1957.                                                 Lin, L.-J. Self-improving reactive agents based on reinforce-
-                                                                   ment learning, planning and teaching. Machine learning,
-Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,           8(3-4):293–321, 1992.
-   Schulman, J., Tang, J., and Zaremba, W. Openai gym,
-   2016.                                                        Mannor, S. and Tsitsiklis, J. N. Mean-variance optimization
-                                                                   in markov decision processes. In International Confer-
-Dhariwal, P., Hesse, C., Plappert, M., Radford, A., Schul-         ence on Machine Learning, pp. 177–184, 2011.
-   man, J., Sidor, S., and Wu, Y. Openai baselines. https:
-  //github.com/openai/baselines, 2017.                          Mannor, S., Simester, D., Sun, P., and Tsitsiklis, J. N. Bias
-                                                                   and variance approximation in value function estimates.
-Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,           Management Science, 53(2):308–322, 2007.
-  V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,
-   I., et al. Impala: Scalable distributed deep-rl with impor-  Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
-   tance weighted actor-learner architectures. arXiv preprint     J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
-   arXiv:1802.01561, 2018.                                         land, A. K., Ostrovski, G., et al. Human-level control
-                                                                   through deep reinforcement learning. Nature, 518(7540):
-Fox, R., Pakman, A., and Tishby, N. Taming the noise in            529–533, 2015.
-   reinforcement learning via soft updates. In Proceedings of
-   the Thirty-Second Conference on Uncertainty in Artiﬁcial     Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
-  Intelligence, pp. 202–211. AUAI Press, 2016.                    T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
-                                                                   chronous methods for deep reinforcement learning. In
-Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft           International Conference on Machine Learning, pp. 1928–
-   actor-critic: Off-policy maximum entropy deep reinforce-       1937, 2016.
-   ment learning with a stochastic actor. arXiv preprint
-   arXiv:1801.01290, 2018.                                      Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare,
-                                                                   M. Safe and efﬁcient off-policy reinforcement learning.
-He, F. S., Liu, Y., Schwing, A. G., and Peng, J. Learning          In Advances in Neural Information Processing Systems,
-   to play in a day: Faster deep reinforcement learning by         pp. 1054–1062, 2016.
-   optimality tightening. arXiv preprint arXiv:1611.01606,
-   2016.                                                        Nachum, O., Norouzi, M., Tucker, G., and Schuurmans, D.
-                                                                   Smoothed action value functions for learning gaussian
-Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup,         policies. arXiv preprint arXiv:1803.02348, 2018.
-   D., and Meger, D. Deep Reinforcement Learning that
-   Matters. arXiv preprint arXiv:1709.06560, 2017.              O’Donoghue, B., Osband, I., Munos, R., and Mnih, V. The
-                                                                   uncertainty bellman equation and exploration. arXiv
-                                                                   preprint arXiv:1709.05380, 2017.
-
-Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel,      Pendrith, M. D., Ryan, M. R., et al. Estimator variance in
-   M., van Hasselt, H., and Silver, D. Distributed prioritized     reinforcement learning: Theoretical problems and practi-
-   experience replay. International Conference on Learning         cal solutions. University of New South Wales, School of
-  Representations, 2018.                                           Computer Science and Engineering, 1997.
-Addressing Function Approximation Error in Actor-Critic Methods
-
-Petrik, M. and Scherrer, B. Biasing approximate dynamic          Van Seijen, H., Van Hasselt, H., Whiteson, S., and Wiering,
-   programming with a lower discount factor. In Advances in         M. A theoretical and empirical analysis of expected sarsa.
-  Neural Information Processing Systems, pp. 1265–1272,             In Adaptive Dynamic Programming and Reinforcement
-   2009.                                                           Learning, 2009. ADPRL’09. IEEE Symposium on, pp.
-                                                                   177–184. IEEE, 2009.
-Popov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-
-   Maron, G., Vecerik, M., Lampe, T., Tassa, Y., Erez,           Watkins, C. J. C. H. Learning from delayed rewards. PhD
-  T., and Riedmiller, M. Data-efﬁcient deep reinforce-              thesis, King’s College, Cambridge, 1989.
-   ment learning for dexterous manipulation. arXiv preprint
-   arXiv:1704.03073, 2017.                                       Wu, Y., Mansimov, E., Grosse, R. B., Liao, S., and Ba,
-                                                                   J. Scalable trust-region method for deep reinforcement
-Precup, D., Sutton, R. S., and Dasgupta, S. Off-policy              learning using kronecker-factored approximation. In Ad-
-   temporal-difference learning with function approxima-            vances in Neural Information Processing Systems, pp.
-   tion. In International Conference on Machine Learning,           5285–5294, 2017.
-   pp. 417–424, 2001.
-
-Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-
-   tized experience replay. In International Conference on
-  Learning Representations, Puerto Rico, 2016.
-
-Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,
-   P. Trust region policy optimization. In International
-  Conference on Machine Learning, pp. 1889–1897, 2015.
-
-Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
-   Klimov, O. Proximal policy optimization algorithms.
-   arXiv preprint arXiv:1707.06347, 2017.
-
-Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
-   Riedmiller, M. Deterministic policy gradient algorithms.
-   In International Conference on Machine Learning, pp.
-   387–395, 2014.
-
-Sutton, R. S. Learning to predict by the methods of temporal
-   differences. Machine learning, 3(1):9–44, 1988.
-
-Sutton, R. S. and Barto, A. G. Reinforcement learning: An
-   introduction, volume 1. MIT press Cambridge, 1998.
-
-Thrun, S. and Schwartz, A. Issues in using function approx-
-   imation for reinforcement learning. In Proceedings of the
-  1993 Connectionist Models Summer School Hillsdale, NJ.
-  Lawrence Erlbaum, 1993.
-
-Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics
-   engine for model-based control. In Intelligent Robots
-   and Systems (IROS), 2012 IEEE/RSJ International Con-
-   ference on, pp. 5026–5033. IEEE, 2012.
-
-Uhlenbeck, G. E. and Ornstein, L. S. On the theory of the
-   brownian motion. Physical review, 36(5):823, 1930.
-
-Van Hasselt, H. Double q-learning. In Advances in Neural
-  Information Processing Systems, pp. 2613–2621, 2010.
-
-Van Hasselt, H., Guez, A., and Silver, D. Deep reinforce-
-   ment learning with double q-learning. In AAAI, pp. 2094–
-   2100, 2016.
-
